{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "The target of project is analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **I94_SAS_Labels_Descriptions.SAS** to get validation dataset. We will use `I94Port.txt` as list of airport, city, state.\n",
    "\n",
    "##### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* I94 Immigration.\n",
    "* World Temperature Data.\n",
    "* I94_SAS_Labels_Descriptions.SAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools will be used and import:\n",
    "- Spark, Spark SQL\n",
    "- Python, Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform dataset assessment use Python script and then review the outputs.\n",
    "```code reference\n",
    "    Describe_and_Gather_Data-submit-03b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perform outputs:\n",
    "- Amount of records and data size: \n",
    "    - I94 Immigration Dataset: `3096313 rows`\n",
    "    - World Temperature Dataset: `8599212 rows`\n",
    "    - i94port SAS Labels Dataset: `660 rows`\n",
    "- Data format included: \n",
    "    - I94 Immigration Dataset is a `.sas7bdat`\n",
    "    - World Temperature Dataset is a `.csv`\n",
    "    - SAS Labels Descriptions is a `.SAS`\n",
    "\n",
    "Our choosen datesets sastify the project rubric and will be using for data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectations :\n",
    "- The choosen datasets enough to perform a data modeling of fact and dimention tables to analysis the relationship between amount of travel immigration and weather duration by month of city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset quality and validation issues recognition from gathering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration dataset:\n",
    "    - `cicid`: Code for visitor origin country. Need to perform uniqueness verification. Have to review datatype. Check NULL values.\n",
    "    - `i94yr | i94mon`: Year, Month of immigration date. Have to review data type. Check NULL or NaN values.\n",
    "    - `i94cit | i94res`: Country of citizenship & Country of recidence. Have to validate these values with `I94_SAS_Labels_Descriptions.SAS`. Check NULL or Nan values.\n",
    "    - `i94port`: Code for destination immigration port of a specific USA city. Have to validate these values with **I94_SAS_Labels_Descriptions.SAS**. There are types of airport that do not allow immigration entry. Check NULL or Nan values.\n",
    "    - `arrdate | depdate`: Arrival date in the USA & Departure date from the USA. Have to review datatype. Check NULL values. Check the dependence between arrival date and departure date. \n",
    "    - `i94mode`: Code for immigration transportation mode. Have to validate these values with **I94_SAS_Labels_Descriptions.SAS**. There are methods of immigration transportation without airport gateway.\n",
    "    - `i94addr`: US state code. Have to validate these values with **I94_SAS_Labels_Descriptions.SAS**.\n",
    "    - `i94bir`: Age of Respondent in Years. Have to review data type. Check NULL or NaN values. We should scope passenger with birth year that keep use airport for their traveling next many year. The analysis will be usefull also.\n",
    "    - `i94visa`: Code for visa type corresponse to visiting reason. Have to validate these values with **I94_SAS_Labels_Descriptions.SAS**.\n",
    "    - count, tadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, insnum: Useless. Do not use these columns.\n",
    "    - `biryear`: Immigrant year of birth. Have to review data type. Check NULL or NaN values. We should scope passenger with birth year that keep use airport for their traveling next many year. The analysis will be usefull also.\n",
    "    - `gender`: Immigrant sex. There are some un-common sex kind. No need these un-common values.\n",
    "    - `airline`: Airline used to arrive in U.S. Have to review data type. Check NULL or NaN values. Check for combination key.\n",
    "    - `admnum`: Admission Number. Have to review data type. Check NULL or NaN values.\n",
    "    - `fltno`: Flight number of Airline used to arrive in U.S. Have to review data type. Check NULL or NaN values.\n",
    "    - `visatype`: Class of admission legally admitting the non-immigrant to temporarily stay in U.S. Have to validate these values with **I94_SAS_Labels_Descriptions.SAS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature dataset\n",
    "    - `dt`: The creation time of temperature. Datatype must be converto datetime. Perform uniqueless verification.\n",
    "    - `AverageTemperature | AverageTemperatureUncertainty`: temperature value recognized. Column name as title style, have to lower this column name.\n",
    "    - `City | Country`: City of Country that the teperature recognized. Column name as title style, have to lower this column name.\n",
    "    - `Latitude | Longitude`: Geographical location in lat-long. Helpful for heatmap but these columns is useless in our project.\n",
    "    - Character case of column names mixed of upper, lower, whitespace. Have to lower case and replace whitespaces with '_'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *I94_SAS_Labels_Descriptions.SAS* extracted `i94port` (with column names will be used later)\n",
    "    - `i94port_valid_code`: airport code.\n",
    "    - `i94port_city_name`: the city corresponding to airport code.\n",
    "    - `i94port_state_code`: the state the city belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse *I94_SAS_Labels_Descriptions.SAS*. for validations on staging steps\n",
    "    ```code reference\n",
    "        Extract_I94_SAS_Labels-v03d.ipynb\n",
    "    ```\n",
    "    - The outputs of this step: `i94_port.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning I94 Immigration dataset\n",
    "\n",
    "    ```code reference\n",
    "        unitTest-cleaning_staging_i94.ipynb\n",
    "    ```\n",
    "    The outputs of this step: `i94immi_df_clean.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning World Temperature dataset\n",
    "\n",
    "    ```code reference\n",
    "        unitTest-cleaning_staging_world_tempe.ipynb\n",
    "    ```\n",
    "    The outputs of this step: `worldtempe_df_clean.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table:\n",
    "- As expectation mention, we want to find out the relations between US immigration with either weather, immigration traffic and the arrival place (city).\n",
    "- The fact table `fact_immi_ưeather` should includes columns:\n",
    "    - `traveller_cicid`\n",
    "    - `arr_airport_code`\n",
    "    - `arr_city`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `arr_datetime_iso`\n",
    "    - `arr_year`\n",
    "    - `arr_month`\n",
    "    - `arr_state_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` \n",
    "    - `immi_datetime_iso`\n",
    "    - `arr_port_code`\n",
    "    - `travel_city`\n",
    "    - `travel_month`\n",
    "    - `travel_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `tempe_month`\n",
    "    - `tempe_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code`\n",
    "    - `city_name`\n",
    "    - `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year`\n",
    "    - `arrival_month`\n",
    "    - `arrival_date`\n",
    "        - dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw dataset from source into Spark dataframe: df_spark_i94, df_spark_dem and df_spark_temp for one month.\n",
    "- Clean each Spark dataframe as decscibed in *Step 2 Cleaning steps* and write each cleaned dataframe into parquet as staging table: stage_i94_immigration, stage_cities_demographics and stage_uscities_temperatures.\n",
    "- Create and load dimension tables: dim_us_ports, dim_visa, dim_countries, dim_travelmode and dim_demographics.\n",
    "- Create and load fact table fact_i94_visits joining stage_i94_immigration and stage_uscities_temperatures.\n",
    "- Create and load dimension tables and dim_date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "* i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration\n",
    "* AverageTemperature = average temperature of destination city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * Spark was chosen since it can easily handle multiple file formats (including SAS) containing large amounts of data. \n",
    "    * Spark SQL was chosen to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    * The data should be updated monthly in conjunction with the current raw file format.\n",
    "    * Case 2\n",
    "    * Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "    * If the data was increased by 100x, we would no longer process the data as a single batch job. We could perhaps do incremental updates using a tool such as Uber's Hudi. We could also consider moving Spark to cluster mode using a cluster manager such as Yarn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * If the data needs to populate a dashboard daily to meet an SLA then we could use a scheduling tool such as Airflow to run the ETL pipeline overnight.\n",
    "    * Others solution ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    * If the database needed to be accessed by 100+ people, we could consider publishing the parquet files to HDFS and giving read access to users that need it. \n",
    "    * If the users want to run SQL queries on the raw data, we could consider publishing to HDFS using a tool such as Impala."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.test_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "af440d615888d659e90fddc0bb1ee33d8fb1bc777d369b970824ddfe4cd12e65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
