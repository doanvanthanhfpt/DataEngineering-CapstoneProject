{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **Airport Code**: This is a simple table with airport codes. The source of this data is from https://datahub.io/core/airport-codes#data. It is highly recommended to use it for educational purpose only but not for commercial or any other purpose. This data is already uploaded to the workspace.\n",
    "\n",
    "* **U.S. City Demographic Data**: This data comes from OpenSoft link https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "* Other text files such as * *I94Addr.txt* *, * *I94CIT_I94RES.txt* *, * *I94Mode.txt* *, * *I94Port.txt* * and * *I94Visa.txt* * files are used to enrich immigration data for better analysis. These files are created from the * *I94_SAS_Labels_Descriptions.SAS* * file provided to describe each and every field in the immigration data.\n",
    "\n",
    "**The project follows the follow steps**:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Scope \n",
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* I94 Immigration\n",
    "* World Temperature Data\n",
    "* U.S. City Demographic Data\n",
    "* Airport Code\n",
    "* Other reference *I94_SAS_Labels_Descriptions.SAS*.\n",
    "\n",
    "Tools will be used:\n",
    "- Spark\n",
    "- Python and its modules\n",
    "- Pandas\n",
    "- AWS Redshift cluster will be considered as an optional to run one or more ETL steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Describe and Gather Data\n",
    "\n",
    "Take a overview on datas will be using for data modeling. Data description information include schema, sample record, number of rows, number of data file (if need).\n",
    "Firstly, we create a config file for parameter of directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data assessment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counting number of records and data size. Take a look on schema, data column structure, attributes and some of sample records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overview of analysis target\n",
    "\n",
    "    Processes datasources **Immigration Data**, **Temperature Data**, **U.S. City Demographic Data** and **Airport Code Table**, to create a star schema optimized for queries on international travel and tourism statistics. This includes a fact table and dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset quality and validation issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Airport Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other reference *I94_SAS_Labels_Descriptions.SAS*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse *I94_SAS_Labels_Descriptions.SAS*. for validations on staging steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging I94 Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging  Airport Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table:\n",
    "- Fact table fact_i94visits will contain informations from the i94 immigration data joined with daily average temperature on the port city and arrival date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension table:\n",
    "- dim_us_ports contains informations like US port of entry code, city, state code and state name.\n",
    "- dim_visa maps visa type which gives information like reason for visiting.\n",
    "- dim_countries tells which country does visitor come from.\n",
    "- dim_travelmode gives mode of transportation: air, land or sea.\n",
    "- dim_demographicscontains median age and population informations about US port city.\n",
    "- dim_date contains date information like year, month, day, week of year and weekday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw dataset from source into Spark dataframe: df_spark_i94, df_spark_dem and df_spark_temp for one month.\n",
    "- Clean each Spark dataframe as decscibed in *Step 2 Cleaning steps* and write each cleaned dataframe into parquet as staging table: stage_i94_immigration, stage_cities_demographics and stage_uscities_temperatures.\n",
    "- Create and load dimension tables: dim_us_ports, dim_visa, dim_countries, dim_travelmode and dim_demographics.\n",
    "- Create and load fact table fact_i94_visits joining stage_i94_immigration and stage_uscities_temperatures.\n",
    "- Create and load dimension tables and dim_date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "* i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration\n",
    "* AverageTemperature = average temperature of destination city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * Spark was chosen since it can easily handle multiple file formats (including SAS) containing large amounts of data. \n",
    "    * Spark SQL was chosen to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    * The data should be updated monthly in conjunction with the current raw file format.\n",
    "    * Case 2\n",
    "    * Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "    * If the data was increased by 100x, we would no longer process the data as a single batch job. We could perhaps do incremental updates using a tool such as Uber's Hudi. We could also consider moving Spark to cluster mode using a cluster manager such as Yarn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * If the data needs to populate a dashboard daily to meet an SLA then we could use a scheduling tool such as Airflow to run the ETL pipeline overnight.\n",
    "    * Others solution ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    * If the database needed to be accessed by 100+ people, we could consider publishing the parquet files to HDFS and giving read access to users that need it. \n",
    "    * If the users want to run SQL queries on the raw data, we could consider publishing to HDFS using a tool such as Impala."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "353a41f28dcf25de28982b14843c0032adfea1e677d886eb1397b3db5a1fcd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
