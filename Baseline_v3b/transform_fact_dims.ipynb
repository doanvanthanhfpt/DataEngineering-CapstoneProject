{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['etl.cfg']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data_source = config.get('DIR','INPUT_DIR')\n",
    "output_processed_data = config.get('DIR','OUTPUT_DIR')\n",
    "\n",
    "i94immi_dataset = config.get('DATA','I94_IMMI')\n",
    "worldtempe_dataset = config.get('DATA','WORLD_TEMPE')\n",
    "citydemo_dataset = config.get('DATA','CITY_DEMOGRAPHIC')\n",
    "airport_dataset = config.get('DATA','AIR_PORT')\n",
    "saslabel_dataset = config.get('DATA','SAS_LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session - Using for droduction only\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "            .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "func =  udf (lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Transform to dim_datetime tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Read out from staging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- I94 Immigration staging table: `i94immi_table`\n",
    "- World Temperature staging table: `worldtempe_table`\n",
    "- I94PORT staging table `i94port_table` from SAS_Labels_Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Fact table `fact_immi_weather` wraps informations from datasets to analyze relation between traveller traffic and weather on a specific city.\n",
    "- Dim table of date that immigration happen `dim_datetime`.\n",
    "- Dim table of airport that immigration allows `dim_port`.\n",
    "- Dim table of immigration records `dim_immi_traveller`.\n",
    "- Dim table of measure times `dim_us_temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|    cicid| i94yr|i94mon|       arrival_date|i94res|i94port|arrdate|i94addr|     departure_date|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|5341351.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341352.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341353.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341354.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341355.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341356.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NY|2016-05-08 00:00:00|\n",
      "|5341357.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-01 00:00:00|\n",
      "|5341358.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-02 00:00:00|\n",
      "|5341359.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-07 00:00:00|\n",
      "|5341367.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|   null|2016-04-30 00:00:00|\n",
      "|5341368.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|   null|2016-04-30 00:00:00|\n",
      "|5341371.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-05-13 00:00:00|\n",
      "|5341372.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-05-13 00:00:00|\n",
      "|5341373.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-06-01 00:00:00|\n",
      "|5341374.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341375.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341376.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341377.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341378.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-02 00:00:00|\n",
      "|5341379.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-02 00:00:00|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")\n",
    "i94immi_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- arrival_date: timestamp (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- departure_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table here\n",
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|    cicid| i94yr|i94mon|       arrival_date|i94res|i94port|arrdate|i94addr|     departure_date|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|5341351.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341352.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341353.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cicid,\n",
    "            arrival_date,\n",
    "            YEAR(arrival_date) as i94yr,\n",
    "            MONTH(arrival_date) as i94mon,\n",
    "            i94res,\n",
    "            i94port,\n",
    "            arrdate,\n",
    "            i94addr,\n",
    "            departure_date\n",
    "        FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+------+------+-------+-------+-------+-------------------+\n",
      "|    cicid|       arrival_date|i94yr|i94mon|i94res|i94port|arrdate|i94addr|     departure_date|\n",
      "+---------+-------------------+-----+------+------+-------+-------+-------+-------------------+\n",
      "|5341351.0|2016-04-28 00:00:00| 2016|     4| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341352.0|2016-04-28 00:00:00| 2016|     4| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341353.0|2016-04-28 00:00:00| 2016|     4| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "+---------+-------------------+-----+------+------+-------+-------+-------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|            2465314|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94immi_rows\n",
    "    FROM i94immi_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|        dt|averagetemperature|averagetemperatureuncertainty|   city|dt_converted|      country|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|1960-02-01|             4.995|                        0.325|ABILENE|  1960-02-01|United States|\n",
      "|1960-03-01| 8.575000000000001|                        0.303|ABILENE|  1960-03-01|United States|\n",
      "|1960-04-01|            18.452|                        0.282|ABILENE|  1960-04-01|United States|\n",
      "|1960-05-01|            21.709|          0.28600000000000003|ABILENE|  1960-05-01|United States|\n",
      "|1960-06-01|            27.714|                        0.387|ABILENE|  1960-06-01|United States|\n",
      "|1960-07-01|            27.646|                        0.326|ABILENE|  1960-07-01|United States|\n",
      "|1960-08-01|            27.481|                        0.341|ABILENE|  1960-08-01|United States|\n",
      "|1960-09-01|            24.413|                        0.241|ABILENE|  1960-09-01|United States|\n",
      "|1960-10-01|18.929000000000002|          0.28600000000000003|ABILENE|  1960-10-01|United States|\n",
      "|1960-11-01|            12.531|                        0.235|ABILENE|  1960-11-01|United States|\n",
      "|1960-12-01|             4.114|                        0.214|ABILENE|  1960-12-01|United States|\n",
      "|1961-01-01|             3.827|                        0.337|ABILENE|  1961-01-01|United States|\n",
      "|1961-02-01|              7.63|                        0.349|ABILENE|  1961-02-01|United States|\n",
      "|1961-03-01|12.799000000000001|                        0.329|ABILENE|  1961-03-01|United States|\n",
      "|1961-04-01|            17.142|                        0.397|ABILENE|  1961-04-01|United States|\n",
      "|1961-05-01|            22.582|                        0.254|ABILENE|  1961-05-01|United States|\n",
      "|1961-06-01|             24.53|                        0.333|ABILENE|  1961-06-01|United States|\n",
      "|1961-07-01|            25.783|                         0.24|ABILENE|  1961-07-01|United States|\n",
      "|1961-08-01|26.052000000000003|                        0.205|ABILENE|  1961-08-01|United States|\n",
      "|1961-09-01|22.875999999999998|                        0.391|ABILENE|  1961-09-01|United States|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df = spark.read.csv(\"worldtempe_df_clean.csv\", header=True)\n",
    "worldtempe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- averagetemperature: string (nullable = true)\n",
      " |-- averagetemperatureuncertainty: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- dt_converted: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>averagetemperature</th>\n",
       "      <th>averagetemperatureuncertainty</th>\n",
       "      <th>city</th>\n",
       "      <th>dt_converted</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-02-01</td>\n",
       "      <td>4.995</td>\n",
       "      <td>0.325</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>1960-02-01</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-03-01</td>\n",
       "      <td>8.575000000000001</td>\n",
       "      <td>0.303</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>1960-03-01</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960-04-01</td>\n",
       "      <td>18.452</td>\n",
       "      <td>0.282</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>1960-04-01</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1960-05-01</td>\n",
       "      <td>21.709</td>\n",
       "      <td>0.28600000000000003</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>1960-05-01</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1960-06-01</td>\n",
       "      <td>27.714</td>\n",
       "      <td>0.387</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>1960-06-01</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt averagetemperature averagetemperatureuncertainty     city  \\\n",
       "0  1960-02-01              4.995                         0.325  ABILENE   \n",
       "1  1960-03-01  8.575000000000001                         0.303  ABILENE   \n",
       "2  1960-04-01             18.452                         0.282  ABILENE   \n",
       "3  1960-05-01             21.709           0.28600000000000003  ABILENE   \n",
       "4  1960-06-01             27.714                         0.387  ABILENE   \n",
       "\n",
       "  dt_converted        country  \n",
       "0   1960-02-01  United States  \n",
       "1   1960-03-01  United States  \n",
       "2   1960-04-01  United States  \n",
       "3   1960-05-01  United States  \n",
       "4   1960-06-01  United States  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtempe_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'string'),\n",
       " ('averagetemperature', 'string'),\n",
       " ('averagetemperatureuncertainty', 'string'),\n",
       " ('city', 'string'),\n",
       " ('dt_converted', 'string'),\n",
       " ('country', 'string')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtempe_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "worldtempe_df = worldtempe_df.withColumn(\"averagetemperature\", worldtempe_df[\"averagetemperature\"].cast(DoubleType()).alias(\"averagetemperature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|        dt|averagetemperature|averagetemperatureuncertainty|   city|dt_converted|      country|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|1960-02-01|             4.995|                        0.325|ABILENE|  1960-02-01|United States|\n",
      "|1960-03-01| 8.575000000000001|                        0.303|ABILENE|  1960-03-01|United States|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df = worldtempe_df.withColumn('dt_converted', func(col('dt_converted')))\n",
    "worldtempe_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'string'),\n",
       " ('averagetemperature', 'double'),\n",
       " ('averagetemperatureuncertainty', 'string'),\n",
       " ('city', 'string'),\n",
       " ('dt_converted', 'date'),\n",
       " ('country', 'string')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtempe_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# worldtempe_table here\n",
    "worldtempe_df.createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|amount_worldtempe_rows|\n",
      "+----------------------+\n",
      "|                165508|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_worldtempe_rows\n",
    "    FROM worldtempe_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            dt_converted,\n",
    "            MONTH(worldtempe_table.dt_converted) as tempe_month,\n",
    "            YEAR(worldtempe_table.dt_converted) as tempe_year,\n",
    "            dt,\n",
    "            city,\n",
    "            averagetemperature,\n",
    "            averagetemperatureuncertainty\n",
    "        FROM worldtempe_table\n",
    "            \"\"\").createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+----------+-------+------------------+-----------------------------+\n",
      "|dt_converted|tempe_month|tempe_year|        dt|   city|averagetemperature|averagetemperatureuncertainty|\n",
      "+------------+-----------+----------+----------+-------+------------------+-----------------------------+\n",
      "|  1960-02-01|          2|      1960|1960-02-01|ABILENE|             4.995|                        0.325|\n",
      "|  1960-03-01|          3|      1960|1960-03-01|ABILENE| 8.575000000000001|                        0.303|\n",
      "|  1960-04-01|          4|      1960|1960-04-01|ABILENE|            18.452|                        0.282|\n",
      "+------------+-----------+----------+----------+-------+------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM worldtempe_table\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        tempe_month,\n",
    "        BROUND(AVG(averagetemperature),2) as averagetemperature,\n",
    "        BROUND(AVG(averagetemperatureuncertainty),2) as averagetemperatureuncertainty,\n",
    "        tempe_year,\n",
    "        dt_converted\n",
    "    FROM worldtempe_table\n",
    "    GROUP BY city, tempe_month, tempe_year, dt_converted\n",
    "\"\"\").createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Quality check dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+-----------------------------+----------+------------+\n",
      "|       city|tempe_month|averagetemperature|averagetemperatureuncertainty|tempe_year|dt_converted|\n",
      "+-----------+-----------+------------------+-----------------------------+----------+------------+\n",
      "|ALBUQUERQUE|          4|              8.71|                         0.36|      1970|  1970-04-01|\n",
      "|  ALLENTOWN|          4|              8.72|                         0.18|      1984|  1984-04-01|\n",
      "|  ANCHORAGE|          4|             -1.29|                         0.43|      1987|  1987-04-01|\n",
      "+-----------+-----------+------------------+-----------------------------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM worldtempe_table\n",
    "    WHERE tempe_month == 4\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|i94port_valid_code|   i94port_city_name|i94port_state_code|\n",
      "+------------------+--------------------+------------------+\n",
      "|               ALC|               ALCAN|                AK|\n",
      "|               ANC|           ANCHORAGE|                AK|\n",
      "|               BAR|BAKER AAF - BAKER...|                AK|\n",
      "|               DAC|       DALTONS CACHE|                AK|\n",
      "|               PIZ|DEW STATION PT LA...|                AK|\n",
      "|               DTH|        DUTCH HARBOR|                AK|\n",
      "|               EGL|               EAGLE|                AK|\n",
      "|               FRB|           FAIRBANKS|                AK|\n",
      "|               HOM|               HOMER|                AK|\n",
      "|               HYD|               HYDER|                AK|\n",
      "|               JUN|              JUNEAU|                AK|\n",
      "|               5KE|           KETCHIKAN|                AK|\n",
      "|               KET|           KETCHIKAN|                AK|\n",
      "|               MOS|MOSES POINT INTER...|                AK|\n",
      "|               NIK|             NIKISKI|                AK|\n",
      "|               NOM|                 NOM|                AK|\n",
      "|               PKC|         POKER CREEK|                AK|\n",
      "|               ORI|      PORT LIONS SPB|                AK|\n",
      "|               SKA|             SKAGWAY|                AK|\n",
      "|               SNP|     ST. PAUL ISLAND|                AK|\n",
      "+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94port_staging\")\n",
    "i94port_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94port_valid_code: string (nullable = true)\n",
      " |-- i94port_city_name: string (nullable = true)\n",
      " |-- i94port_state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94port_table\n",
    "i94port_df.createOrReplaceTempView('i94port_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94port_rows|\n",
      "+-------------------+\n",
      "|                583|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94port_rows\n",
    "    FROM i94port_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|i94port_valid_code|   i94port_city_name|i94port_state_code|\n",
      "+------------------+--------------------+------------------+\n",
      "|               ALC|               ALCAN|                AK|\n",
      "|               ANC|           ANCHORAGE|                AK|\n",
      "|               BAR|BAKER AAF - BAKER...|                AK|\n",
      "+------------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94port_table\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create dim and fact tables from staging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "# month, year???\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            immi94.arrival_date as arrival_date\n",
    "        FROM i94immi_table as immi94\n",
    "        LEFT JOIN worldtempe_table as wt\n",
    "                ON wt.dt_converted = immi94.arrival_date\n",
    "            \"\"\").createOrReplaceTempView('dim_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            dim_datetime.arrival_date,\n",
    "            MONTH(dim_datetime.arrival_date) as arrival_month, \n",
    "            YEAR(dim_datetime.arrival_date) as arrival_year\n",
    "        FROM dim_datetime\n",
    "            \"\"\").createOrReplaceTempView('dim_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------------+\n",
      "|       arrival_date|arrival_month|arrival_year|\n",
      "+-------------------+-------------+------------+\n",
      "|2016-04-28 00:00:00|            4|        2016|\n",
      "|2016-04-28 00:00:00|            4|        2016|\n",
      "|2016-04-28 00:00:00|            4|        2016|\n",
      "+-------------------+-------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_datetime\n",
    "            \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "====================================================================\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            i94port_table.i94port_valid_code as port_code,\n",
    "            i94port_table.i94port_city_name as city_name, \n",
    "            i94port_table.i94port_state_code as state\n",
    "        FROM i94port_table\n",
    "            \"\"\").createOrReplaceTempView('dim_port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+\n",
      "|port_code|           city_name|state|\n",
      "+---------+--------------------+-----+\n",
      "|      ALC|               ALCAN|   AK|\n",
      "|      ANC|           ANCHORAGE|   AK|\n",
      "|      BAR|BAKER AAF - BAKER...|   AK|\n",
      "+---------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_port\n",
    "            \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "====================================================================\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "# dim_datetime as ddate\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            immi94.cicid as immi_cicid,\n",
    "            ddate.arrival_date as immi_datetime_iso,\n",
    "            ddate.arrival_month as travel_month,\n",
    "            ddate.arrival_year as travel_year,\n",
    "            immi94.i94port as arr_port_code\n",
    "        FROM i94immi_table as immi94\n",
    "        JOIN dim_datetime as ddate\n",
    "            ON ddate.arrival_month = immi94.i94mon\n",
    "            \"\"\").createOrReplaceTempView('dim_immi_traveller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------+-----------+-------------+\n",
      "|immi_cicid|  immi_datetime_iso|travel_month|travel_year|arr_port_code|\n",
      "+----------+-------------------+------------+-----------+-------------+\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|          DAL|\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|          DAL|\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|          DAL|\n",
      "+----------+-------------------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_immi_traveller\n",
    "            \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "# dim_datetime as ddate\n",
    "# dim_port as dport\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            d_travel.immi_cicid as immi_cicid,\n",
    "            d_travel.immi_datetime_iso as immi_datetime_iso,\n",
    "            d_travel.travel_month as travel_month,\n",
    "            d_travel.travel_year as travel_year,\n",
    "            dport.city_name as travel_city,\n",
    "            d_travel.arr_port_code as arr_port_code\n",
    "        FROM dim_port as dport\n",
    "        JOIN dim_immi_traveller as d_travel\n",
    "            ON dport.port_code = d_travel.arr_port_code\n",
    "     \"\"\").createOrReplaceTempView('dim_immi_traveller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------+-----------+-----------+-------------+\n",
      "|immi_cicid|  immi_datetime_iso|travel_month|travel_year|travel_city|arr_port_code|\n",
      "+----------+-------------------+------------+-----------+-----------+-------------+\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|     DALLAS|          DAL|\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|     DALLAS|          DAL|\n",
      "| 5341351.0|2016-04-28 00:00:00|           4|       2016|     DALLAS|          DAL|\n",
      "+----------+-------------------+------------+-----------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_immi_traveller\n",
    "            \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "====================================================================\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "# dim_datetime as ddate\n",
    "# dim_port as dport\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            ddate.arrival_date as collected_datetime,\n",
    "            ddate.arrival_month as tempe_month,\n",
    "            wt.averagetemperature as avg_tempe,\n",
    "            wt.averagetemperatureuncertainty as avg_uncertain_tempe,\n",
    "            wt.city as city_tempe_collect\n",
    "        FROM dim_datetime as ddate\n",
    "        LEFT JOIN worldtempe_table as wt\n",
    "            ON wt.tempe_month = ddate.arrival_month\n",
    "     \"\"\").createOrReplaceTempView('dim_us_temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------+-------------------+------------------+\n",
      "| collected_datetime|tempe_month|avg_tempe|avg_uncertain_tempe|city_tempe_collect|\n",
      "+-------------------+-----------+---------+-------------------+------------------+\n",
      "|2016-04-28 00:00:00|          4|     9.02|               0.28|       WESTMINSTER|\n",
      "|2016-04-28 00:00:00|          4|    10.73|               0.32|  WEST VALLEY CITY|\n",
      "|2016-04-28 00:00:00|          4|     7.11|               0.28|  WEST VALLEY CITY|\n",
      "|2016-04-28 00:00:00|          4|     11.3|                0.4|       WEST JORDAN|\n",
      "|2016-04-28 00:00:00|          4|     11.2|               0.28|       WEST JORDAN|\n",
      "+-------------------+-----------+---------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_us_temperature\n",
    "     \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "====================================================================\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load to fact_immi_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            d_travel.immi_cicid as traveller_cicid,\n",
    "            d_travel.arr_port_code as arr_port_code,\n",
    "            d_port.state as arr_state_code,\n",
    "            d_travel.travel_city as arr_city,\n",
    "            d_travel.travel_month as arr_month,\n",
    "            d_travel.travel_year as arr_year\n",
    "        FROM dim_immi_traveller as d_travel\n",
    "        JOIN dim_port as d_port\n",
    "            ON d_port.port_code = d_travel.arr_port_code\n",
    "     \"\"\").createOrReplaceTempView('fact_immi_weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "|traveller_cicid|arr_port_code|arr_state_code|arr_city|arr_month|arr_year|\n",
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM fact_immi_weather\n",
    "     \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "|traveller_cicid|arr_port_code|arr_state_code|arr_city|arr_month|arr_year|\n",
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "|      5341351.0|          DAL|            TX|  DALLAS|        4|    2016|\n",
      "+---------------+-------------+--------------+--------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            fact.traveller_cicid as traveller_cicid,\n",
    "            fact.arr_port_code as arr_port_code,\n",
    "            fact.arr_state_code as arr_state_code,\n",
    "            fact.arr_city as arr_city,\n",
    "            fact.arr_month as arr_month,\n",
    "            fact.arr_year as arr_year\n",
    "        FROM fact_immi_weather as fact\n",
    "     \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            fact.traveller_cicid as traveller_cicid,\n",
    "            fact.arr_port_code as arr_port_code,\n",
    "            fact.arr_state_code as arr_state_code,\n",
    "            fact.arr_city as arr_city,\n",
    "            d_tempe.avg_tempe as avg_tempe,\n",
    "            d_tempe.avg_uncertain_tempe as avg_uncertain_tempe,\n",
    "            fact.arr_month as arr_month,\n",
    "            fact.arr_year as arr_year\n",
    "        FROM fact_immi_weather as fact\n",
    "        LEFT JOIN dim_us_temperature as d_tempe\n",
    "            ON d_tempe.city_tempe_collect = fact.arr_city\n",
    "     \"\"\").createOrReplaceTempView('fact_immi_weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o174.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 420, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:554)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:554)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8c13ffa58bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mFROM\u001b[0m \u001b[0mfact_immi_weather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m      \"\"\").show(5)\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o174.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 420, localhost, executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:554)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:554)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM fact_immi_weather\n",
    "     \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f19dfd6b1cd7fd360d4f2c4802461aa893d068ea99183b3eab6718091575a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
