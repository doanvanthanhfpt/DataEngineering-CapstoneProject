{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['etl.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data_source = config.get('DIR','INPUT_DIR')\n",
    "output_processed_data = config.get('DIR','OUTPUT_DIR')\n",
    "\n",
    "i94immi_dataset = config.get('DATA','I94_IMMI')\n",
    "worldtempe_dataset = config.get('DATA','WORLD_TEMPE')\n",
    "citydemo_dataset = config.get('DATA','CITY_DEMOGRAPHIC')\n",
    "airport_dataset = config.get('DATA','AIR_PORT')\n",
    "saslabel_dataset = config.get('DATA','SAS_LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session - Using for droduction only\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "            .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Transform to dim tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Read out from staging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|        dt|averagetemperature|averagetemperatureuncertainty|   city|dt_converted|      country|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "|1960-02-01|             4.995|                        0.325|ABILENE|  1960-02-01|United States|\n",
      "|1960-03-01| 8.575000000000001|                        0.303|ABILENE|  1960-03-01|United States|\n",
      "|1960-04-01|            18.452|                        0.282|ABILENE|  1960-04-01|United States|\n",
      "|1960-05-01|            21.709|          0.28600000000000003|ABILENE|  1960-05-01|United States|\n",
      "|1960-06-01|            27.714|                        0.387|ABILENE|  1960-06-01|United States|\n",
      "|1960-07-01|            27.646|                        0.326|ABILENE|  1960-07-01|United States|\n",
      "|1960-08-01|            27.481|                        0.341|ABILENE|  1960-08-01|United States|\n",
      "|1960-09-01|            24.413|                        0.241|ABILENE|  1960-09-01|United States|\n",
      "|1960-10-01|18.929000000000002|          0.28600000000000003|ABILENE|  1960-10-01|United States|\n",
      "|1960-11-01|            12.531|                        0.235|ABILENE|  1960-11-01|United States|\n",
      "|1960-12-01|             4.114|                        0.214|ABILENE|  1960-12-01|United States|\n",
      "|1961-01-01|             3.827|                        0.337|ABILENE|  1961-01-01|United States|\n",
      "|1961-02-01|              7.63|                        0.349|ABILENE|  1961-02-01|United States|\n",
      "|1961-03-01|12.799000000000001|                        0.329|ABILENE|  1961-03-01|United States|\n",
      "|1961-04-01|            17.142|                        0.397|ABILENE|  1961-04-01|United States|\n",
      "|1961-05-01|            22.582|                        0.254|ABILENE|  1961-05-01|United States|\n",
      "|1961-06-01|             24.53|                        0.333|ABILENE|  1961-06-01|United States|\n",
      "|1961-07-01|            25.783|                         0.24|ABILENE|  1961-07-01|United States|\n",
      "|1961-08-01|26.052000000000003|                        0.205|ABILENE|  1961-08-01|United States|\n",
      "|1961-09-01|22.875999999999998|                        0.391|ABILENE|  1961-09-01|United States|\n",
      "+----------+------------------+-----------------------------+-------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df = spark.read.csv(\"worldtempe_df_clean.csv\", header=True)\n",
    "worldtempe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- averagetemperature: string (nullable = true)\n",
      " |-- averagetemperatureuncertainty: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- dt_converted: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "worldtempe_df.createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|             165508|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_worldtempe_rows\n",
    "    FROM worldtempe_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|    cicid| i94yr|i94mon|       arrival_date|i94res|i94port|arrdate|i94addr|     departure_date|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "|5341351.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341352.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341353.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341354.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341355.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NV|2016-05-03 00:00:00|\n",
      "|5341356.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     NY|2016-05-08 00:00:00|\n",
      "|5341357.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-01 00:00:00|\n",
      "|5341358.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-02 00:00:00|\n",
      "|5341359.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    DAL|20572.0|     TX|2016-05-07 00:00:00|\n",
      "|5341367.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|   null|2016-04-30 00:00:00|\n",
      "|5341368.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|   null|2016-04-30 00:00:00|\n",
      "|5341371.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-05-13 00:00:00|\n",
      "|5341372.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-05-13 00:00:00|\n",
      "|5341373.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NJ|2016-06-01 00:00:00|\n",
      "|5341374.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341375.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341376.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341377.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-01 00:00:00|\n",
      "|5341378.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-02 00:00:00|\n",
      "|5341379.0|2016.0|   4.0|2016-04-28 00:00:00| 575.0|    NEW|20572.0|     NY|2016-05-02 00:00:00|\n",
      "+---------+------+------+-------------------+------+-------+-------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")\n",
    "i94immi_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- arrival_date: timestamp (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- departure_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|            2465314|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94immi_rows\n",
    "    FROM i94immi_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|i94port_valid_code|   i94port_city_name|i94port_state_code|\n",
      "+------------------+--------------------+------------------+\n",
      "|               ALC|               ALCAN|                AK|\n",
      "|               ANC|           ANCHORAGE|                AK|\n",
      "|               BAR|BAKER AAF - BAKER...|                AK|\n",
      "|               DAC|       DALTONS CACHE|                AK|\n",
      "|               PIZ|DEW STATION PT LA...|                AK|\n",
      "|               DTH|        DUTCH HARBOR|                AK|\n",
      "|               EGL|               EAGLE|                AK|\n",
      "|               FRB|           FAIRBANKS|                AK|\n",
      "|               HOM|               HOMER|                AK|\n",
      "|               HYD|               HYDER|                AK|\n",
      "|               JUN|              JUNEAU|                AK|\n",
      "|               5KE|           KETCHIKAN|                AK|\n",
      "|               KET|           KETCHIKAN|                AK|\n",
      "|               MOS|MOSES POINT INTER...|                AK|\n",
      "|               NIK|             NIKISKI|                AK|\n",
      "|               NOM|                 NOM|                AK|\n",
      "|               PKC|         POKER CREEK|                AK|\n",
      "|               ORI|      PORT LIONS SPB|                AK|\n",
      "|               SKA|             SKAGWAY|                AK|\n",
      "|               SNP|     ST. PAUL ISLAND|                AK|\n",
      "+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94port_staging\")\n",
    "i94port_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- i94port_valid_code: string (nullable = true)\n",
      " |-- i94port_city_name: string (nullable = true)\n",
      " |-- i94port_state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94port_df.createOrReplaceTempView('i94port_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94port_rows|\n",
      "+-------------------+\n",
      "|                583|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94port_rows\n",
    "    FROM i94port_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "====================================================================\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input 'as' expecting <EOF>(line 4, pos 27)\\n\\n== SQL ==\\n\\n        SELECT \\n            immi.i94addr as state_code,\\n        FROM i94immi_table as immi94\\n---------------------------^^^\\n            JOIN i94port_table as port\\n                ON port.i94port_state_code = immi94.i94addr\\n            \\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input 'as' expecting <EOF>(line 4, pos 27)\n\n== SQL ==\n\n        SELECT \n            immi.i94addr as state_code,\n        FROM i94immi_table as immi94\n---------------------------^^^\n            JOIN i94port_table as port\n                ON port.i94port_state_code = immi94.i94addr\n            \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5f1375dca997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mJOIN\u001b[0m \u001b[0mi94port_table\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mON\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi94port_state_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimmi94\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi94addr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input 'as' expecting <EOF>(line 4, pos 27)\\n\\n== SQL ==\\n\\n        SELECT \\n            immi.i94addr as state_code,\\n        FROM i94immi_table as immi94\\n---------------------------^^^\\n            JOIN i94port_table as port\\n                ON port.i94port_state_code = immi94.i94addr\\n            \\n\""
     ]
    }
   ],
   "source": [
    "# i94immi_table as immi94\n",
    "# worldtempe_table as wt\n",
    "# i94port_table as port\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            immi.i94addr as state_code,\n",
    "        FROM i94immi_table as immi94\n",
    "            JOIN i94port_table as port\n",
    "                ON port.i94port_state_code = immi94.i94addr\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create tables from staging dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f19dfd6b1cd7fd360d4f2c4802461aa893d068ea99183b3eab6718091575a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
