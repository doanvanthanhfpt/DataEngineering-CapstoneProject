{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Project 5\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **Airport Code**: This is a simple table with airport codes. The source of this data is from https://datahub.io/core/airport-codes#data. It is highly recommended to use it for educational purpose only but not for commercial or any other purpose. This data is already uploaded to the workspace.\n",
    "\n",
    "* Other text files such as * *I94Addr.txt* *, * *I94CIT_I94RES.txt* *, * *I94Mode.txt* *, * *I94Port.txt* * and * *I94Visa.txt* * files are used to enrich immigration data for better analysis. These files are created from the * *I94_SAS_Labels_Descriptions.SAS* * file provided to describe each and every field in the immigration data.\n",
    "\n",
    "**The project follows the follow steps**:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scope \n",
    "\n",
    "Processes datasources Immigration Data, Temperature Data and Airport Code Table, to create a star schema optimized for queries on international travel and tourism statistics. This includes a fact table and dimension tables.\n",
    "\n",
    "Spark, Python modules (pyspark, os, pandas) are using for this project to to steps garthering, exploring, cleaning, modeling, pipeline creating for ETL building on local system. AWS Redshift cluster will be considered as an optional to run one or more ETL steps. \n",
    "\n",
    "- Fact Table\n",
    "    <tbd>\n",
    "    \n",
    "- Dimension Tables\n",
    "    <tbd>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe and Gather Data\n",
    "\n",
    "Take a overview on datas will be using for data modeling. Data description information include schema, sample record, number of rows, number of data file (if need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "# df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "# write to parquet\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "# df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines procedure get data description informations\n",
    "def gather_datasource(input_datasource):\n",
    "    '''\n",
    "    A procedure that returns ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_datasource : str\n",
    "        name of the ...\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ...\n",
    "        ...\n",
    "    '''\n",
    "\n",
    "    filename_from_fname = os.path.basename(input_datasource).split('/')[-1]\n",
    "    split_tup = os.path.splitext(filename_from_fname)\n",
    "    file_name = split_tup[0]\n",
    "    file_ext = split_tup[1]\n",
    "\n",
    "    print(\"Datasource File Name: \", file_name)\n",
    "    print(\"Datasource File Extension: \", file_ext)\n",
    "    \n",
    "    # in each line remove unnecessary spaces and extract the code and its corresponding value \n",
    "    if '.csv' in file_ext:\n",
    "        df = spark.read.csv(input_datasource,header='True')\n",
    "        print('The schema: ')\n",
    "        df.printSchema()\n",
    "        print()\n",
    "        print('Sample records: ')\n",
    "        df.show(5)\n",
    "        print()\n",
    "        print('Total rows: ', df.count())\n",
    "        print()\n",
    "    elif '.sas7bdat' in file_ext:\n",
    "        df = spark.read.format('com.github.saurfang.sas.spark').load(input_datasource)\n",
    "        print('The schema: ')\n",
    "        df.printSchema()\n",
    "        print()\n",
    "        print('Sample records: ')\n",
    "        df.show(5)\n",
    "        print()\n",
    "        print('Total rows: ', df.count())\n",
    "        print()\n",
    "    else :\n",
    "        print(\"Datasource file extension {} will be update later. Finish here!!!!\".format(file_ext))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines procedure count data files\n",
    "def count_datafile(input_datasource):\n",
    "    path, filename = os.path.split(input_datasource)\n",
    "    file_list = os.listdir(path)\n",
    "    count_file = 0\n",
    "    print(\"Files and directories in '\", path, \"' :\")\n",
    "    for file_name in file_list:\n",
    "        print(file_name)\n",
    "        count_file += 1\n",
    "\n",
    "    print()\n",
    "    print('Total data files:', count_file)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garthering I94 Immigration Data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "gather_datasource(fname)\n",
    "count_datafile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garthering World Temperature Data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "gather_datasource(fname)\n",
    "count_datafile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garthering Airport Code Table\n",
    "fname = './airport-codes_csv.csv'\n",
    "gather_datasource(fname)\n",
    "count_datafile(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the Data\n",
    "\n",
    "Data quality issues:\n",
    "\n",
    "- Missing or empty or wrong values\n",
    "- Duplicate data\n",
    "- Upcase, lowercase, space mixed\n",
    "- Wrong format values\n",
    "- NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing from here\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_clean = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    appName('CleanSteps').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines procedure split data file to chunks\n",
    "def split_data(input_datasource, parent_dir):\n",
    "    dir_file, file_name_ext = os.path.split(input_datasource)\n",
    "    # dir_file without / at the end\n",
    "    # file_name with extension\n",
    "    \n",
    "    filename_from_fname = os.path.basename(input_datasource).split('/')[-1]\n",
    "    split_tup = os.path.splitext(filename_from_fname)\n",
    "    file_name = split_tup[0] # file name without extension\n",
    "    file_ext = split_tup[1] # file extension\n",
    "\n",
    "    chunking_name = file_name + '_part'\n",
    "    chunk_size=100000\n",
    "    numbering_batch=1\n",
    "\n",
    "    if '.sas7bdat' in file_ext:\n",
    "        directory = file_ext\n",
    "        full_dir = os.path.join(parent_dir, directory)\n",
    "        if not os.path.exists(full_dir):\n",
    "            os.mkdir(full_dir)\n",
    "            print(\"Directory \" , full_dir,  \" Created \")\n",
    "        else:\n",
    "            print(\"Directory \" , full_dir,  \" already exists. Clean existing directory and run again.\")\n",
    "            exit(1)\n",
    "        # split .sas7bdat file\n",
    "        for batch in pd.read_sas(input_datasource, encoding=\"ISO-8859-1\", chunksize=chunk_size):\n",
    "            batch.to_csv(full_dir + '/' + chunking_name + str(numbering_batch) + '.csv', index=False)\n",
    "            numbering_batch += 1\n",
    "    elif '.csv' in file_ext:\n",
    "        directory = file_ext\n",
    "        full_dir = os.path.join(parent_dir, directory)\n",
    "        if not os.path.exists(full_dir):\n",
    "            os.mkdir(full_dir)\n",
    "            print(\"Directory \" , full_dir,  \" Created \")\n",
    "        else:\n",
    "            print(\"Directory \" , full_dir,  \" already exists. Clean existing directory and run again.\")\n",
    "            exit(1)\n",
    "        # split .csv file\n",
    "        for batch in pd.read_csv(input_datasource, chunksize=chunk_size):\n",
    "            batch.to_csv(full_dir + '/' + chunking_name + str(numbering_batch) + '.csv', index=False)\n",
    "            numbering_batch += 1\n",
    "    else :\n",
    "            print('Datasource type have not update yet')\n",
    "    print(\"Separated to batchs: \")\n",
    "    return (numbering_batch - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_dir(parent_dir,input_datasource):\n",
    "    filename_from_fname = os.path.basename(input_datasource).split('/')[-1]\n",
    "    split_tup = os.path.splitext(filename_from_fname)\n",
    "    file_ext = split_tup[1] # file extension\n",
    "\n",
    "    return (parent_dir + file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Matching immigration data with i94port.txt\n",
    "reg_exp_ops = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_i94port = {}\n",
    "list_i94port_valid_code = '/home/workspace/i94port.txt'\n",
    "with open(list_i94port_valid_code) as f:\n",
    "    for port_name in f:\n",
    "        matching_port = reg_exp_ops.search(port_name)\n",
    "        valid_i94port[matching_port[1]]=[matching_port[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function filter out .csv chunk data\n",
    "def clean_i94_csv_data(input_datasource):\n",
    "    df_immi = spark_clean.read.csv(input_datasource,inferSchema=True,header=True)\n",
    "    df_immi = df_immi.filter(df_immi.i94port.isin(list(valid_i94port.keys())))\n",
    "\n",
    "    return df_immi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run clean_i94_csv_data function\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "home_dir = '/home/workspace/enrich_data/'\n",
    "immi_chunk_dir = get_chunk_dir(home_dir, fname)\n",
    "chunk_name = immi_chunk_dir + 'GlobalLandTemperaturesByCity_part9.csv'\n",
    "\n",
    "df_clean_csv = clean_i94_csv_data(chunk_name)\n",
    "df_clean_csv.select(df_clean_csv.i94port).show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function filter out .sas7bdat data\n",
    "def clean_i94_sas7bdat_data(input_datasource):\n",
    "    df_immigration = spark_clean.read.format('com.github.saurfang.sas.spark').load(input_datasource)\n",
    "    df_immigration = df_immigration.filter(df_immigration.i94port.isin(list(valid_i94port.keys())))\n",
    "\n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clean_i94_sas7bdat_data function\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "home_dir = '/home/workspace/enrich_data/'\n",
    "immi_chunk_dir = get_chunk_dir(home_dir, fname)\n",
    "chunk_name = immi_chunk_dir + 'i94_apr16_sub_part1.csv'\n",
    "\n",
    "df_clean_sas7bdat = clean_i94_sas7bdat_data(fname)\n",
    "df_clean_sas7bdat.select(df_clean_sas7bdat.i94port).show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft note - Do not run this block\n",
    "\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "fname_dir = '../../data/18-83510-I94-Data-2016'\n",
    "\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "fname_dir = '../../data2'\n",
    "\n",
    "fname = './airport-codes_csv.csv'\n",
    "fname_dir = '.'\n",
    "\n",
    "df = spark.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "df.printSchema()\n",
    "df.count()\n",
    "\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df = pd.read_csv(fname, sep=',')\n",
    "df.head()\n",
    "\n",
    "# input data source airport-codes_csv.csv\n",
    "airport_input_data = \"./airport-codes_csv.csv\"\n",
    "# Parse csv file\n",
    "airport_df = pd.read_csv(airport_input_data, sep=',')\n",
    "# Verify airport-codes_csv.csv parsed as dataframe\n",
    "airport_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Conceptual Data Model\n",
    "\n",
    "(Writing a little of data modeling here)\n",
    "\n",
    "Result Table - I94 immigration data joined with the city temperature data on i94port, Columns:\n",
    "\n",
    "* i94yr = 4 digit year,\n",
    "* i94mon = numeric month,\n",
    "* i94cit = 3 digit code of origin city,\n",
    "* i94port = 3 character code of destination USA city,\n",
    "* arrdate = arrival date in the USA,\n",
    "* i94mode = 1 digit travel code,\n",
    "* depdate = departure date from the USA,\n",
    "* i94visa = reason for immigration,\n",
    "* AverageTemperature = average temperature of destination city,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Data dictionary\n",
    "\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "    * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "353a41f28dcf25de28982b14843c0032adfea1e677d886eb1397b3db5a1fcd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
