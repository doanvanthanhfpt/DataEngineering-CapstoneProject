{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create environment variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# Create Spark session\n",
    "def spark_session_init():\n",
    "    spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID)\\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\",AWS_SECRET_ACCESS_KEY)\\\n",
    "            .enableHiveSupport().getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Procedure read out validation pair values from SAS Labels Description\n",
    "def get_validation_code_from_SAS_labels(sas_input_label):\n",
    "    '''\n",
    "    This procedure read a input SAS Labels Description and then write out validation code datasets.\n",
    "    The SAS Labels Description included validation code datasets with labels: I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sas_input_label : string\n",
    "        The label name of validation code dataset. Its can be one of I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    validation_code_list : validation_value_pairs(tuple(str_valid_code, str_valid_value))\n",
    "        The return output is a specific SAS label list of validation code value pairs.\n",
    "    '''\n",
    "\n",
    "    # Read input SAS Labels Descriptions\n",
    "    with open('I94_SAS_Labels_Descriptions.SAS') as sas_validation_code:\n",
    "            labels_from_sas = sas_validation_code.read()\n",
    "\n",
    "    # Parse labels from SAS Label Description input\n",
    "    sas_labels = labels_from_sas[labels_from_sas.index(sas_input_label):]\n",
    "    sas_labels = sas_labels[:sas_labels.index(';')]\n",
    "    \n",
    "    # Processing line by line, remove separate charaters and then append value pair\n",
    "    lines = sas_labels.splitlines()\n",
    "    validation_code_list = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            valid_code, valid_value = line.split('=')\n",
    "            valid_code = valid_code.strip().strip(\"'\").strip('\"')\n",
    "            valid_value = valid_value.strip().strip(\"'\").strip('\"').strip()\n",
    "            validation_code_list.append((valid_code, valid_value))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return validation_code_list\n",
    "\n",
    "# Procedure extract parts from SAS Labels Description\n",
    "def extract_staging_sas_label(spark, label):\n",
    "    \"\"\"\n",
    "    Process dataset 'I94_SAS_Labels_Descriptions.SAS' and output to dataframe.\n",
    "    \n",
    "    Steps of dataset processing:\n",
    "    - Extract data from 'I94_SAS_Labels_Descriptions.SAS'.\n",
    "    - Transform to dataframe.\n",
    "        \n",
    "    Keyword arguments:\n",
    "        * label_str | label name as string datatype.\n",
    "\n",
    "    Output:\n",
    "        Return dataframe for next step.\n",
    "    \"\"\"\n",
    "    label_name = label\n",
    "    valid_code = label + \"_valid_code\"\n",
    "    valid_value = label + \"_valid_value\"\n",
    "    csv_output = label + \"_sas_label_validation\"\n",
    "    \n",
    "    # Create dir for output\n",
    "    parent_dir = \"./\"\n",
    "    path = os.path.join(parent_dir, csv_output)\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" % csv_output)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\" % csv_output)\n",
    "\n",
    "    # Define output dataframe structure\n",
    "    schema = R([\n",
    "        Fld(valid_code, Str()),\n",
    "        Fld(valid_value, Str())\n",
    "    ])\n",
    "\n",
    "    # Create dataframe from extracted label\n",
    "    df = spark.createDataFrame(\n",
    "        data=get_validation_code_from_SAS_labels(label_name),\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # df.write.mode('overwrite').csv(csv_output)\n",
    "    shutil.rmtree(csv_output, ignore_errors=False, onerror=None)\n",
    "    df.write.options(header='True', delimiter=',').csv(csv_output)\n",
    "\n",
    "    df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(csv_output)\n",
    "\n",
    "    print(\"Top 20 rows of {} \".format(csv_output))\n",
    "    df.show()\n",
    "\n",
    "    print(\"Count rows of {}: {} \".format(csv_output, df.count()))\n",
    "    \n",
    "    print(\"Check unique value of {}: {} \".format(csv_output, df.select(valid_code).distinct().count()))\n",
    "\n",
    "    print(\"Staging csv files in: {}\".format(csv_output))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Procedure convert column name\n",
    "def convert_column_names(df):\n",
    "    '''\n",
    "    This procedure standardizing column names to snake case format. \n",
    "    Format ex: customer_name, billing_address, total_price.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : string_of_dataframe\n",
    "        The input dataframe with column names might have elements of:\n",
    "            - Messy columns names\n",
    "            - Including accents\n",
    "            - Different delimiters\n",
    "            - Casing and multiple white spaces.\n",
    "        Snake case style replaces the white spaces and symbol delimiters\n",
    "          with underscore and converts all characters to lower case.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with column names has been changed to snake_case format.\n",
    "    '''\n",
    "    cols = df.columns\n",
    "    column_name_changed = []\n",
    "\n",
    "    for col in cols:\n",
    "        new_column = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")\n",
    "        column_name_changed.append(new_column)\n",
    "\n",
    "    df.columns = column_name_changed\n",
    "\n",
    "# Procedure remove specific dir (if need)\n",
    "def rmdir(directory):\n",
    "    '''\n",
    "    This procedure perform pure recursive a directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string_of_path_to_dir\n",
    "        The input directory is a path to target dir. \n",
    "        This dir and all its belong child objects wil be deleted.\n",
    "\n",
    "        Syntax note: rmdir(Path(\"target_path_to_dir\"))\n",
    "            with Path(\"target_path_to_dir\") returns path to dir format as 'directory' input\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    directory = Path(directory)\n",
    "    for item in directory.iterdir():\n",
    "        if item.is_dir():\n",
    "            rmdir(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "    directory.rmdir()\n",
    "\n",
    "# Defines procedure count data files - Done\n",
    "def get_list_of_files(dir_name):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dir_name)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dir_name, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + get_list_of_files(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_outputs = './ws_parquet_outputs'\n",
    "spark = spark_session_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting I94 Immigration dataset (from './sas_data').\")\n",
    "i94immi_df = spark.read.parquet('./sas_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and staging I94 Immigration dataset\n",
    "print(\"Transforming I94 Immigration dataset:\")\n",
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop amount of un-makesance records cause by `DepartureDate >= ArrivalDate`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE arrdate <= depdate\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Add column `arrival_date = timestone + arrdate_offset_day`, with:\n",
    "# - timestone = '1960-01-01' (***datetime*** datatype)\n",
    "# - arrdate_offset_day = 'arrdate' (***integer*** datatype)\n",
    "# - arrival_date (***datetime*** datatype)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date \n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Add column `departure_date = timestone + depdate_offset_day`, with:\n",
    "# - `timestone` = '1960-01-01' (***datetime*** datatype)\n",
    "# - `depdate_offset_day` = 'depdate' (***integer*** datatype)\n",
    "# - `departure_date` (***datetime*** datatype)\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= arrdate THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'NaN' END AS departure_date \n",
    "                FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Extracted i94mode from `I94_SAS_Labels_Descriptions_SAS`\n",
    "# i94mode includes:\n",
    "# {'1': 'Air', '2': 'Sea', '3': 'Land', '9': 'Not reported'}\n",
    "# Keep air arrival only, mean keep `i94mode=1`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94mode == 1.0\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Mapping `i94visa` numbers to `visatype` instead\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, CASE \n",
    "                WHEN i94visa = 1.0 THEN 'Business' \n",
    "                WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                WHEN i94visa = 3.0 THEN 'Student'\n",
    "                ELSE 'NaN' END AS visa_type\n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Keep user records of `male = 'M'` and `female = 'F'` only\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM i94immi_table \n",
    "    WHERE gender IN ('F', 'M')\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Drop NULL value on arrival state\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94addr IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")\n",
    "\n",
    "# Keep necessary columns\n",
    "# Convert month and year timestamp\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cicid,\n",
    "        i94cit,\n",
    "        i94res,\n",
    "        i94port,\n",
    "        arrival_date,\n",
    "        YEAR(arrival_date) as i94yr,\n",
    "        MONTH(arrival_date) as i94mon,\n",
    "        i94mode,\n",
    "        i94addr,\n",
    "        departure_date,\n",
    "        i94bir,\n",
    "        i94visa,\n",
    "        count,\n",
    "        dtadfile,\n",
    "        biryear,\n",
    "        dtaddto,\n",
    "        gender,\n",
    "        insnum,\n",
    "        airline,\n",
    "        admnum,\n",
    "        fltno,\n",
    "        visatype,\n",
    "        visa_type\n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline the cleaned\n",
    "i94immi_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning staging output location\n",
    "path = './i94immi_df_clean' # Use local storage\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "\n",
    "rmdir(Path(\"i94immi_df_clean\")) # use this line from 2nd running\n",
    "\n",
    "# Staging to csv file\n",
    "i94immi_df.write.options(header='True', delimiter=',').csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out from staging\n",
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'fact_i94immi'\n",
    "# Transform to Spark SQL TempView\n",
    "fact_i94immi_df = i94immi_df\n",
    "fact_i94immi_df.createOrReplaceTempView('fact_i94immi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact table fact_i94immi\n",
    "fact_i94immi = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        cicid as travel_cicid,\n",
    "        i94cit as from_country_code,\n",
    "        i94res as immi_country_code,\n",
    "        i94port as arrival_port_code,\n",
    "        arrival_date as immi_arrival_date,\n",
    "        i94yr as arrival_year,\n",
    "        i94mon as arrival_month,\n",
    "        i94mode as airline_mode_code,\n",
    "        i94addr as immi_state_code,\n",
    "        departure_date,\n",
    "        i94bir as traveller_age,\n",
    "        i94visa as visatype_by_number,\n",
    "        biryear as traveller_birth_year,\n",
    "        gender as traveller_sex,\n",
    "        fltno as immi_flight_code,\n",
    "        visatype as visatype_by_code,\n",
    "        visa_type\n",
    "    FROM fact_i94immi\n",
    "\"\"\")\n",
    "\n",
    "# Save table to parquet files\n",
    "fact_i94immi_parquet_outputs = parquet_outputs + '/fact_i94immi.parquet'\n",
    "fact_i94immi.write.mode(\"overwrite\").parquet(fact_i94immi_parquet_outputs)\n",
    "\n",
    "# fact_i94immi parquet files inventory\n",
    "print(\"'fact_i94immi' parquet files inventory\")\n",
    "list_of_files = get_list_of_files(fact_i94immi_parquet_outputs)\n",
    "for item in list_of_files:\n",
    "    print(item)\n",
    "print (\"Parquet files inventory finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i94immi_dataset_handling(spark, input_dataset, parquet_outputs):\n",
    "    \"\"\"\n",
    "    Process I94 Immigration dataset (from './sas_data').\n",
    "    \n",
    "    Steps of dataset processing:\n",
    "    - Extract data from dataset.\n",
    "    - Transform data: cleaning -> staging -> output to csv for later.\n",
    "    - Load to star data modeling fact & dim tables.\n",
    "    \n",
    "    Keyword arguments:\n",
    "        * spark                 | Spark session.\n",
    "        * processed_outputs     | path to output dir\n",
    "\n",
    "    Output:\n",
    "        The output of fact&dim tables saved out to parquet files in corresponding:\n",
    "        * fact_i94immi\n",
    "        * dim_visa\n",
    "        * dim_immi_flight\n",
    "        * dim_immi_travaller\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"===== ETL processing - I94 Immigration =====\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # Loading to fact & dim tables\n",
    "    print(\"Loading I94 Immigration dataset to fact & dim tables:\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Dir definitions\n",
    "    parquet_outputs = './ws_parquet_outputs'\n",
    "\n",
    "    # Define Spark session initilization\n",
    "    spark = spark_session_init()\n",
    "\n",
    "    # Dataset definitions\n",
    "    i94immi_dataset = 'sas_data'\n",
    "\n",
    "    # ETL processes - done\n",
    "    i94immi_dataset_handling(spark, i94immi_dataset, parquet_outputs)\n",
    "    \n",
    "    # All output parquet parts and files inventory - Done\n",
    "    print(\"All parquet parts and files inventory\")\n",
    "    list_of_files = get_list_of_files(parquet_outputs)\n",
    "    for item in list_of_files:\n",
    "        print(item)\n",
    "    print (\"All parquet parts and files inventory finished\")\n",
    "\n",
    "    # \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
