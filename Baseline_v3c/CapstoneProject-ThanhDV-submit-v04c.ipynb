{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "The target of project is analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **I94_SAS_Labels_Descriptions.SAS** to get validation dataset. We will use `I94Port.txt` as list of airport, city, state.\n",
    "\n",
    "##### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* I94 Immigration.\n",
    "* World Temperature Data.\n",
    "* I94_SAS_Labels_Descriptions.SAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools will be used and import:\n",
    "- Spark, Spark SQL\n",
    "- Python, Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform dataset assessment use Jupiter Notebook and then review the outputs.\n",
    "```code reference\n",
    "    Describe_and_Gather_Data-submit-v03c.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perform outputs:\n",
    "- Amount of records and data size: \n",
    "    - I94 Immigration Dataset: `3096313 rows`\n",
    "    - World Temperature Dataset: `8599212 rows`\n",
    "    - i94port SAS Labels Dataset: `660 rows`\n",
    "- Data file extension formats included: \n",
    "    - I94 Immigration Dataset is a `.sas7bdat`\n",
    "    - World Temperature Dataset is a `.csv`\n",
    "    - SAS Labels Descriptions is a `.SAS`\n",
    "\n",
    "Our choosen datesets sastify the project rubric and will be using for data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectations :\n",
    "- The choosen datasets enough to perform a data modeling of fact and dimention tables to analysis the relationship between amount of travel immigration and weather duration by month of city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset quality and validation issues recognition from gathering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration dataset:\n",
    "    - `cicid`: Visitor US cid code issue on every travller get throught the immigration port.\n",
    "    - `i94yr | i94mon`: Year, Month of immigration date.\n",
    "    - `i94cit | i94res`: Country of Citizenship & Country of recidence. From `I94_SAS_Labels_Descriptions.SAS`. Don't use.\n",
    "    - `i94port`: Port code for a specific immigration port USA city. From `I94_SAS_Labels_Descriptions.SAS`. There are types of airport that do not allow immigration entry.\n",
    "    - `arrdate | depdate`: Arrival date in the USA & Departure date from the USA.\n",
    "    - `i94mode`: Code for immigration transportation mode. From `I94_SAS_Labels_Descriptions.SAS`. There are methods of immigration transportation without airport gateway. Don't use.\n",
    "    - `i94addr`: US state code. From `I94_SAS_Labels_Descriptions.SAS`.\n",
    "    - `i94bir`: Age of traveller in Years. Don't use.\n",
    "    - `i94visa`: Code for visa type corresponse to visiting reason. Don't use.\n",
    "    - `count, tadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, insnum`: Don't use.\n",
    "    - `biryear`: Immigrant year of birth. Have to review data type. Don't use.\n",
    "    - `gender`: Immigrant sex. There are some un-common sex kind. Don't use.\n",
    "    - `airline`: Airline Coporate used to arrive in U.S. Don't use.\n",
    "    - `admnum`: Admission Number. Don't use.\n",
    "    - `fltno`: Flight number of Airline used to arrive in U.S. Don't use.\n",
    "    - `visatype`: Class of admission legally admitting the non-immigrant to temporarily stay in U.S. Don't use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature dataset\n",
    "    - `dt`: The creation time of temperature.\n",
    "    - `AverageTemperature | AverageTemperatureUncertainty`: temperature value recognized.\n",
    "    - `City | Country`: City of Country that the teperature recognized.\n",
    "    - `Latitude | Longitude`: Geographical location in lat-long. Helpful for heatmap but these columns is useless in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *I94_SAS_Labels_Descriptions.SAS* extracted `i94port` (with column names will be used later)\n",
    "    - `i94port_valid_code`: airport code.\n",
    "    - `i94port_city_name`: the city corresponding to airport code.\n",
    "    - `i94port_state_code`: the state the city belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning `i94port` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "- The outputs of this step: `i94_port.csv`\n",
    "- Cleaning task to do: \n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "    - Clean leading and trailing white space.\n",
    "    - Split to port_code, city, state.\n",
    "    - Clean others columns to limit dataset.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        Extract_I94_SAS_Labels-v03d.ipynb\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning I94 Immigration dataset of a month `../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat`\n",
    "- The outputs of this step: `i94immi_df_clean.csv`\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to spark dataframe.\n",
    "    - Create Spark SQL table from dataframe.\n",
    "    - Choose Primarykey.\n",
    "    - Verify arrival date and departure date logical conditional.\n",
    "    - Add column `arival_date`, `departure_date` as `datetime` datatype\n",
    "    - Verify `arival_date`, `departure_date` wrong value.\n",
    "    - Filter US airport with immigration allowed.\n",
    "    - Remove missing value.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        cleaning_staging_i94immi-v3c.ipynb\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning World Temperature dataset `../../data2/GlobalLandTemperaturesByCity.csv`\n",
    "- The outputs of this step: `worldtempe_df_clean.csv`.\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to pandas dataframe.\n",
    "    - Filter value of `United States` only.\n",
    "    - Limit dataset duration by immigration time duration\n",
    "    - Clean column with datetime datatype.\n",
    "    - Standalizing column names format.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        cleaning_staging_worldtempe-v3c.ipynb\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expectation mention, we want to find out the relations between US immigration with either weather, immigration traffic and the arrival place (city). To archive the expectation, we create star data modeling with fact and dim tables detail bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table:\n",
    "- The fact table `fact_immi_weather` should includes columns:\n",
    "    - `traveller_cicid`\n",
    "    - `arr_airport_code`\n",
    "    - `arr_city`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `arr_datetime_iso`\n",
    "    - `arr_year`\n",
    "    - `arr_month`\n",
    "    - `arr_state_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` \n",
    "    - `immi_datetime_iso`\n",
    "    - `arr_port_code`\n",
    "    - `travel_city`\n",
    "    - `travel_month`\n",
    "    - `travel_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `tempe_month`\n",
    "    - `tempe_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code`\n",
    "    - `city_name`\n",
    "    - `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year`\n",
    "    - `arrival_month`\n",
    "    - `arrival_date`\n",
    "    - dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw dataset from source into dataframes:\n",
    "    - I94 Immigration to `i94immi_df` as Spark dataframe.\n",
    "    - WWorld Temperature to `worldtempe_df` as Pandas dataframe.\n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS` as a Spark dataframe.\n",
    "- Describe and Gather Data on:\n",
    "    - `i94immi_df` as Spark dataframe.\n",
    "    - `worldtempe_df` as Pandas dataframe.\n",
    "    - `I94PORT` as a Spark dataframe.\n",
    "- Clean each Spark dataframe as decscibed to staging dataset:\n",
    "    - `i94immi_df` cleaned output to `i94immi_df_clean` as a csv format.\n",
    "    - `worldtempe_df` cleaned output to `worldtempe_df_clean` as a csv format.\n",
    "    - `I94PORT` cleaned output to `i94port_staging` as a csv format.\n",
    "- Transform csv staging datasets to staging tables:\n",
    "    - `i94immi_df_clean` staging to `i94immi_table` as Spark SQL table.\n",
    "    - `worldtempe_df_clean` cleaned output to `worldtempe_table` as Spark SQL table.\n",
    "    - `i94port_staging` cleaned output to `i94port_table` as Spark SQL table.\n",
    "- Transform staging tables to fact and dim tables.\n",
    "    - `dim_immi_travller` is transformed from `i94immi_table`, `dim_port`.\n",
    "    - `dim_us_temperature` is transformed from `worldtempe_table` and `dim_datetime`.\n",
    "    - `dim_datetime` is transformed from `i94immi_table`.\n",
    "    - `dim_port` is transformed from `i94port_table`.\n",
    "    - `fact_immi_weather` is loaded from dim tables.\n",
    "- Create and run pipeline to model data.\n",
    "    - Load raw datasets; Describe and Gather Data: `Describe_and_Gather_Data-submit-03b.ipynb`\n",
    "    - Clean and staging datasets:\n",
    "        - `Extract_I94_SAS_Labels-v03d.ipynb`\n",
    "        - `cleaning_staging_i94immi-v3c.ipynb`\n",
    "        - `cleaning_staging_worldtempe-v3c.ipynb`\n",
    "    - Transform to fact and dim tables:\n",
    "        - `transform_fact_dims-v03c.ipynb`\n",
    "- Create data quality check for fact and dim tables.\n",
    "    - For dim tables `quality_check_dims.ipynb` (not yet)\n",
    "    - For fact tables `quality_check_fact.ipynb` (not yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Create the data model\n",
    "Run steps of pipeline\n",
    "\n",
    "- `Describe_and_Gather_Data-submit-03b.ipynb`\n",
    "- `Extract_I94_SAS_Labels-v03d.ipynb`\n",
    "- `cleaning_staging_i94immi-v3c.ipynb`\n",
    "- `cleaning_staging_worldtempe-v3c.ipynb`\n",
    "- `transform_fact_dims-v03c.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fact table `fact_immi_weather` should includes columns:\n",
    "    - `traveller_cicid` as `varchar` datatype\n",
    "    - `arr_airport_code` as `varchar` datatype\n",
    "    - `arr_city` as `varchar` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `arr_datetime_iso` as `datetime` datatype\n",
    "    - `arr_year` as `datetime` datatype\n",
    "    - `arr_month` as `datetime` datatype\n",
    "    - `arr_state_code` as `varchar` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dim tables\n",
    "\n",
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` as `varchar` datatype\n",
    "    - `immi_datetime_iso` as `datetime` datatype\n",
    "    - `arr_port_code` as `varchar` datatype\n",
    "    - `travel_city` as `varchar` datatype\n",
    "    - `travel_month` as `datetime` datatype\n",
    "    - `travel_year` as `datetime` datatype\n",
    "\n",
    "- `dim_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect` as `datetime` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `tempe_month` as `datetime` datatype\n",
    "    - `tempe_year` as `datetime` datatype\n",
    "\n",
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code` as `varchar` datatype\n",
    "    - `city_name` as `varchar` datatype\n",
    "    - `state` as `varchar` datatype\n",
    "\n",
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year` as `datetime` datatype\n",
    "    - `arrival_month` as `datetime` datatype\n",
    "    - `arrival_date` as `datetime` datatype\n",
    "    * dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - Pandas was chosen since it can easily handle dataframe either input or output csv. Easy to install and config Pandas on local computer to push up progress with SDK as VScode or Atom.\n",
    "    - Spark were chosen since capable of handling support file formats (ex. sas7bdat, SAS) with large volume of data.\n",
    "    - Dataframes of Spark or Pandas can be converted to each other.\n",
    "    - Spark SQL was chosen since capable of processing the large input files into dataframes and manipulated via commom SQL JOIN, modify table structure, aggregate data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    - Depending on the purpose of use to make period data analysis suggestions.\n",
    "        * Yearly: To have data for data analysis education\n",
    "        * Monthly: To have a basis for predicting seasonal travller traffic.\n",
    "        * Weekly: To have a basis for serving medical purposes (tracing infectious diseases like COVID, flu, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios the data was increased by 100x:\n",
    "    - For the case that need to be met in terms of periodic timing, 100x is a really big volume to meet target be on time. For this scenarios, we should make design a region distributed big data for collecting, processing, modeling. This distributed big data should be many cluster on cloud-based system. Data will be split to chunks, every chunks being processed by a batch-job.\n",
    "    - For the case no worry about time, we can process the data as a single batch job. We could use existing cloud-based bigdata processing solution as datalake, datawarehouse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For specific time update dashboard visualization, we can apply pipeline automation. Airflow is a good candidate cause of capacities schedule, automation, processing batch-job.\n",
    "    - The configurations of cloud-based processing system is a key factor to speed up the analysis progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    - We could consider publishing the parquet files to read-ony HDFS. \n",
    "    - In scenarios need  to meet high frequency SQL queries we consider place a midleware (ex. redis, memcahe, memsql) layer in the front of database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.test_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "af440d615888d659e90fddc0bb1ee33d8fb1bc777d369b970824ddfe4cd12e65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
