{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "The target of project is analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **I94_SAS_Labels_Descriptions.SAS** to get validation dataset. We will use `I94Port.txt` as list of airport, city, state.\n",
    "\n",
    "##### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* I94 Immigration.\n",
    "* World Temperature Data.\n",
    "* I94_SAS_Labels_Descriptions.SAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Tools will be used and import:\n",
    "- Spark, Spark SQL\n",
    "- Python, Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Describe and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- I94 Immigration dataset:\n",
    "    - `cicid`: Visitor US cid code issue on every travller get throught the immigration port.\n",
    "    - `i94yr | i94mon`: Year, Month of immigration date.\n",
    "    - `i94cit | i94res`: Country of Citizenship & Country of recidence. From `I94_SAS_Labels_Descriptions.SAS`. Don't use.\n",
    "    - `i94port`: Port code for a specific immigration port USA city. From `I94_SAS_Labels_Descriptions.SAS`. There are types of airport that do not allow immigration entry.\n",
    "    - `arrdate | depdate`: Arrival date in the USA & Departure date from the USA.\n",
    "    - `i94mode`: Code for immigration transportation mode. From `I94_SAS_Labels_Descriptions.SAS`. There are methods of immigration transportation without airport gateway. Don't use.\n",
    "    - `i94addr`: US state code. From `I94_SAS_Labels_Descriptions.SAS`.\n",
    "    - `i94bir`: Age of traveller in Years. Don't use.\n",
    "    - `i94visa`: Code for visa type corresponse to visiting reason. Don't use.\n",
    "    - `count, tadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, insnum`: Don't use.\n",
    "    - `biryear`: Immigrant year of birth. Have to review data type. Don't use.\n",
    "    - `gender`: Immigrant sex. There are some un-common sex kind. Don't use.\n",
    "    - `airline`: Airline Coporate used to arrive in U.S. Don't use.\n",
    "    - `admnum`: Admission Number. Don't use.\n",
    "    - `fltno`: Flight number of Airline used to arrive in U.S. Don't use.\n",
    "    - `visatype`: Class of admission legally admitting the non-immigrant to temporarily stay in U.S. Don't use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- World Temperature dataset\n",
    "    - `dt`: The creation time of temperature.\n",
    "    - `AverageTemperature | AverageTemperatureUncertainty`: temperature value recognized.\n",
    "    - `City | Country`: City of Country that the teperature recognized.\n",
    "    - `Latitude | Longitude`: Geographical location in lat-long. Helpful for heatmap but these columns is useless in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- I94_SAS_Labels_Descriptions.SAS contains information parts:\n",
    "    - `I94PORT_sas_label_validation` (to be used later)\n",
    "        - `i94port_valid_code`: airport code.\n",
    "        - `i94port_city_name`: the city corresponding to airport code.\n",
    "        - `i94port_state_code`: the state the city belong to.\n",
    "    - `I94MODE_sas_label_validation`\n",
    "        - `i94mode_valid_code`: \n",
    "        - `i94mode_valid_value`:\n",
    "    - `I94VISA_sas_label_validation`\n",
    "        - `i94visa_valid_code`: \n",
    "        - `i94visa_valid_value`:\n",
    "    - `I94ADDR_sas_label_validation`\n",
    "        - `i94addr_valid_code`: \n",
    "        - `i94addr_valid_value`:\n",
    "    - `I94RES_sas_label_validation`\n",
    "        - `i94res_valid_code`: \n",
    "        - `i94res_valid_value`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Perform dataset assessment use Jupiter Notebook and then review the outputs.\n",
    "```code reference\n",
    "    Describe_and_Gather_Data-submit-v03c.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The perform outputs:\n",
    "- Amount of records and data size: \n",
    "    - I94 Immigration Dataset: `3096313 rows`\n",
    "    - World Temperature Dataset: `8599212 rows`\n",
    "    - i94port SAS Labels Dataset: `660 rows`\n",
    "- Data file extension formats included: \n",
    "    - I94 Immigration Dataset is a `.sas7bdat`\n",
    "    - World Temperature Dataset is a `.csv`\n",
    "    - SAS Labels Descriptions is a `.SAS`\n",
    "\n",
    "Our choosen datesets sastify the project rubric and will be using for data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our expectations :\n",
    "- The choosen datasets enough to perform a data modeling of fact and dimention tables to analysis the relationship between amount of travel immigration and weather duration by month of city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Prepare steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session - Using for droduction only\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "            .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure read out validation pair values from SAS Labels Description\n",
    "def get_validation_code_from_SAS_labels(sas_input_label):\n",
    "    '''\n",
    "    This procedure read a input SAS Labels Description and then write out validation code datasets.\n",
    "    The SAS Labels Description included validation code datasets with labels: I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sas_input_label : string\n",
    "        The label name of validation code dataset. Its can be one of I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    validation_code_list : validation_value_pairs(tuple(str_valid_code, str_valid_value))\n",
    "        The return output is a specific SAS label list of validation code value pairs.\n",
    "    '''\n",
    "\n",
    "    # Read input SAS Labels Descriptions\n",
    "    with open('I94_SAS_Labels_Descriptions.SAS') as sas_validation_code:\n",
    "            labels_from_sas = sas_validation_code.read()\n",
    "\n",
    "    # Parse labels from SAS Label Description input\n",
    "    sas_labels = labels_from_sas[labels_from_sas.index(sas_input_label):]\n",
    "    sas_labels = sas_labels[:sas_labels.index(';')]\n",
    "    \n",
    "    # Processing line by line, remove separate charaters and then append value pair\n",
    "    lines = sas_labels.splitlines()\n",
    "    validation_code_list = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            valid_code, valid_value = line.split('=')\n",
    "            valid_code = valid_code.strip().strip(\"'\").strip('\"')\n",
    "            valid_value = valid_value.strip().strip(\"'\").strip('\"').strip()\n",
    "            validation_code_list.append((valid_code, valid_value))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return validation_code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure extract parts from SAS Labels Description\n",
    "def extract_staging_sas_label(label):\n",
    "    '''\n",
    "    asdjhkjf.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    label: \n",
    "        a string input of specific label from \"SAS_Label_Descriptions.SAS\"\n",
    "        \n",
    "    Syntax note: \n",
    "        input value in string datatype, need inside a pair of single quotes. Ex: 'I94RES', 'I94PORTS'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dir of csv files with a specific part as input label.\n",
    "    '''\n",
    "    label_name = label\n",
    "    valid_code = label + \"_valid_code\"\n",
    "    valid_value = label + \"_valid_value\"\n",
    "    csv_output = label + \"_sas_label_validation\"\n",
    "    parent_dir = \"./\"\n",
    "    path = os.path.join(parent_dir, csv_output)\n",
    "    # os.mkdir(path)\n",
    "\n",
    "    schema = R([\n",
    "        Fld(valid_code, Str()),\n",
    "        Fld(valid_value, Str())\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(\n",
    "        data=get_validation_code_from_SAS_labels(label_name),\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    shutil.rmtree(csv_output, ignore_errors=False, onerror=None)\n",
    "    df.write.options(header='True', delimiter=',').csv(csv_output)\n",
    "    # df.write.mode('overwrite').csv(csv_output)\n",
    "\n",
    "    df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(csv_output)\n",
    "\n",
    "    print(\"Top 20 rows of {} \".format(csv_output))\n",
    "    df.show()\n",
    "\n",
    "    print(\"Count rows of {}: {} \".format(csv_output, df.count()))\n",
    "    \n",
    "    print(\"Check unique value of {}: {} \".format(csv_output, df.select(valid_code).distinct().count()))\n",
    "\n",
    "    print(\"Staging csv files in: {}\".format(csv_output))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure convert column name\n",
    "def convert_column_names(df):\n",
    "    '''\n",
    "    This procedure standardizing column names to snake case format. Format ex: customer_name, billing_address, total_price.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : string_of_dataframe\n",
    "        The input dataframe with column names might have elements of messy columns names, including accents, different delimiters, casing and multiple white spaces.\n",
    "        Snake case style replaces the white spaces and symbol delimiters with underscore and converts all characters to lower case\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with column names has been changed to snake_case format.\n",
    "    '''\n",
    "    cols = df.columns\n",
    "    column_name_changed = []\n",
    "\n",
    "    for col in cols:\n",
    "        new_column = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")\n",
    "        column_name_changed.append(new_column)\n",
    "\n",
    "    df.columns = column_name_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure remove specific dir (if need)\n",
    "def rmdir(directory):\n",
    "    '''\n",
    "    This procedure perform pure recursive a directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string_of_path_to_dir\n",
    "        The input directory is a path to target dir. This dir and all its belong child objects wil be deleted.\n",
    "        Syntax note: rmdir(Path(\"target_path_to_dir\"))\n",
    "            with Path(\"target_path_to_dir\") returns path to dir format as 'directory' input\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    directory = Path(directory)\n",
    "    for item in directory.iterdir():\n",
    "        if item.is_dir():\n",
    "            rmdir(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "    directory.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read input dataset to Spark dataframe\n",
    "# i94immi_dataset = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# i94_immi_df = spark.read.format('com.github.saurfang.sas.spark').load(i94immi_dataset)\n",
    "i94immi_df = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'double'),\n",
       " ('i94yr', 'double'),\n",
       " ('i94mon', 'double'),\n",
       " ('i94cit', 'double'),\n",
       " ('i94res', 'double'),\n",
       " ('i94port', 'string'),\n",
       " ('arrdate', 'double'),\n",
       " ('i94mode', 'double'),\n",
       " ('i94addr', 'string'),\n",
       " ('depdate', 'double'),\n",
       " ('i94bir', 'double'),\n",
       " ('i94visa', 'double'),\n",
       " ('count', 'double'),\n",
       " ('dtadfile', 'string'),\n",
       " ('visapost', 'string'),\n",
       " ('occup', 'string'),\n",
       " ('entdepa', 'string'),\n",
       " ('entdepd', 'string'),\n",
       " ('entdepu', 'string'),\n",
       " ('matflag', 'string'),\n",
       " ('biryear', 'double'),\n",
       " ('dtaddto', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'double'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data type\n",
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Attributes columns\n",
    "i94immi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create tableview for data manipulation\n",
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop amount of un-makesance records cause by `DepartureDate >= ArrivalDate`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE arrdate <= depdate\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add column `arrival_date = timestone + arrdate_offset_day`, with:\n",
    "# - timestone = '1960-01-01' (***datetime*** datatype)\n",
    "# - arrdate_offset_day = 'arrdate' (***integer*** datatype)\n",
    "# - arrival_date (***datetime*** datatype)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date \n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add column `departure_date = timestone + depdate_offset_day`, with:\n",
    "# - `timestone` = '1960-01-01' (***datetime*** datatype)\n",
    "# - `depdate_offset_day` = 'depdate' (***integer*** datatype)\n",
    "# - `departure_date` (***datetime*** datatype)\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= arrdate THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'NaN' END AS departure_date \n",
    "                FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracted i94mode from `I94_SAS_Labels_Descriptions_SAS`\n",
    "# i94mode includes:\n",
    "# {'1': 'Air', '2': 'Sea', '3': 'Land', '9': 'Not reported'}\n",
    "# Keep air arrival only, mean keep `i94mode=1`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94mode == 1.0\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Mapping `i94visa` numbers to `visatype` instead\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *, CASE \n",
    "                    WHEN i94visa = 1.0 THEN 'Business' \n",
    "                    WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                    WHEN i94visa = 3.0 THEN 'Student'\n",
    "                    ELSE 'NaN' END AS visa_type\n",
    "        FROM i94immi_table\n",
    "    \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep user records of `male = 'M'` and `female = 'F'` only\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM i94immi_table \n",
    "    WHERE gender IN ('F', 'M')\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop NULL value on arrival state\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94addr IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep necessary columns\n",
    "# Convert month and year timestamp\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cicid,\n",
    "            i94cit,\n",
    "            i94res,\n",
    "            i94port,\n",
    "            arrival_date,\n",
    "            YEAR(arrival_date) as i94yr,\n",
    "            MONTH(arrival_date) as i94mon,\n",
    "            i94mode,\n",
    "            i94addr,\n",
    "            departure_date,\n",
    "            i94bir,\n",
    "            i94visa,\n",
    "            count,\n",
    "            dtadfile,\n",
    "            biryear,\n",
    "            dtaddto,\n",
    "            gender,\n",
    "            insnum,\n",
    "            airline,\n",
    "            admnum,\n",
    "            fltno,\n",
    "            visatype,\n",
    "            visa_type\n",
    "        FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|            2377896|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleaned\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94immi_rows\n",
    "    FROM i94immi_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare to save out s3 storage\n",
    "i94immi_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Staging to csv file\n",
    "rmdir(Path(\"i94immi_df_clean\")) # use this line from 2nd running\n",
    "i94immi_df.write.options(header='True', delimiter=',').csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify out from staging csv partitions\n",
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|       arrival_date|i94yr|i94mon|i94mode|i94addr|     departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     CA|2016-05-08 00:00:00|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     NV|2016-05-17 00:00:00|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-08 00:00:00|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "|5748520.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-14 00:00:00|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1| Business|\n",
      "|5748521.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-14 00:00:00|  28.0|    1.0|  1.0|20160430| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'double'),\n",
       " ('i94cit', 'double'),\n",
       " ('i94res', 'double'),\n",
       " ('i94port', 'string'),\n",
       " ('arrival_date', 'timestamp'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94mode', 'double'),\n",
       " ('i94addr', 'string'),\n",
       " ('departure_date', 'timestamp'),\n",
       " ('i94bir', 'double'),\n",
       " ('i94visa', 'double'),\n",
       " ('count', 'double'),\n",
       " ('dtadfile', 'int'),\n",
       " ('biryear', 'double'),\n",
       " ('dtaddto', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'double'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string'),\n",
       " ('visa_type', 'string')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrival_date',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'departure_date',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'dtadfile',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype',\n",
       " 'visa_type']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Modeling I94 Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94immi_df.createOrReplaceTempView('fact_i94immi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_i94immi = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            cicid as travel_cic_id,\n",
    "            i94cit,\n",
    "            i94res,\n",
    "            i94port as arrival_port_code,\n",
    "            arrival_date,\n",
    "            i94yr as arrival_year,\n",
    "            i94mon as arrival_month,\n",
    "            i94mode,\n",
    "            i94addr,\n",
    "            departure_date,\n",
    "            i94bir as traveller_age,\n",
    "            i94visa,\n",
    "            count,\n",
    "            dtadfile,\n",
    "            biryear as traveller_birth_year,\n",
    "            dtaddto,\n",
    "            gender as traveller_sex,\n",
    "            insnum,\n",
    "            airline,\n",
    "            admnum,\n",
    "            fltno,\n",
    "            visatype,\n",
    "            visa_type\n",
    "        FROM fact_i94immi\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://capstone-project-outputs/s3_outputs/fact_i94immi/fact_i94immi.parquet'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o251.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d70df9eba93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms3_parquet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://capstone-project-outputs/s3_outputs/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'fact_i94immi/fact_i94immi.parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfact_i94immi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_parquet_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o251.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "fact_i94immi.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "World Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read input dataset to Pandas dataframe\n",
    "worldtempe_dataset = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "worldtempe_df = pd.read_csv(worldtempe_dataset,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data type\n",
    "worldtempe_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attributes columns\n",
    "worldtempe_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtempe_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I94PORT_sas_label_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 rows of I94PORT_sas_label_validation \n",
      "+------------------+--------------------+\n",
      "|I94PORT_valid_code| I94PORT_valid_value|\n",
      "+------------------+--------------------+\n",
      "|               ALC|           ALCAN, AK|\n",
      "|               ANC|       ANCHORAGE, AK|\n",
      "|               BAR|BAKER AAF - BAKER...|\n",
      "|               DAC|   DALTONS CACHE, AK|\n",
      "|               PIZ|DEW STATION PT LA...|\n",
      "|               DTH|    DUTCH HARBOR, AK|\n",
      "|               EGL|           EAGLE, AK|\n",
      "|               FRB|       FAIRBANKS, AK|\n",
      "|               HOM|           HOMER, AK|\n",
      "|               HYD|           HYDER, AK|\n",
      "|               JUN|          JUNEAU, AK|\n",
      "|               5KE|       KETCHIKAN, AK|\n",
      "|               KET|       KETCHIKAN, AK|\n",
      "|               MOS|MOSES POINT INTER...|\n",
      "|               NIK|         NIKISKI, AK|\n",
      "|               NOM|             NOM, AK|\n",
      "|               PKC|     POKER CREEK, AK|\n",
      "|               ORI|  PORT LIONS SPB, AK|\n",
      "|               SKA|         SKAGWAY, AK|\n",
      "|               SNP| ST. PAUL ISLAND, AK|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Count rows of I94PORT_sas_label_validation: 660 \n",
      "Check unique value of I94PORT_sas_label_validation: 660 \n",
      "Staging csv files in: I94PORT_sas_label_validation\n"
     ]
    }
   ],
   "source": [
    "# Extract `I94PORT` label\n",
    "# Create dir 'I94PORT_sas_label_validation' to save extracted label result\n",
    "I94PORT_df = extract_staging_sas_label('I94PORT')\n",
    "I94PORT_df = I94PORT_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 660 entries, 0 to 659\n",
      "Data columns (total 2 columns):\n",
      "I94PORT_valid_code     660 non-null object\n",
      "I94PORT_valid_value    660 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Data type\n",
    "I94PORT_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I94PORT_valid_code</th>\n",
       "      <th>I94PORT_valid_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW, AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  I94PORT_valid_code           I94PORT_valid_value\n",
       "0                ALC                     ALCAN, AK\n",
       "1                ANC                 ANCHORAGE, AK\n",
       "2                BAR  BAKER AAF - BAKER ISLAND, AK\n",
       "3                DAC             DALTONS CACHE, AK\n",
       "4                PIZ    DEW STATION PT LAY DEW, AK"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attributes columns\n",
    "I94PORT_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I94PORT_valid_code     660\n",
       "I94PORT_valid_value    660\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count rows\n",
    "I94PORT_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Cleaning I94 Immigration dataset of a month `../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat`\n",
    "- The outputs of this step: `i94immi_df_clean.csv`\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to spark dataframe.\n",
    "    - Create Spark SQL table from dataframe.\n",
    "    - Choose Primarykey.\n",
    "    - Verify arrival date and departure date logical conditional.\n",
    "    - Add column `arival_date`, `departure_date` as `datetime` datatype\n",
    "    - Verify `arival_date`, `departure_date` wrong value.\n",
    "    - Filter US airport with immigration allowed.\n",
    "    - Remove missing value.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        cleaning_staging_i94immi-v3c.ipynb\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "World Temperature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Cleaning World Temperature dataset `../../data2/GlobalLandTemperaturesByCity.csv`\n",
    "- The outputs of this step: `worldtempe_df_clean.csv`.\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to pandas dataframe.\n",
    "    - Filter value of `United States` only.\n",
    "    - Limit dataset duration by immigration time duration\n",
    "    - Clean column with datetime datatype.\n",
    "    - Standalizing column names format.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        cleaning_staging_worldtempe-v3c.ipynb\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I94PORT_sas_label_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Cleaning `i94port` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "- The outputs of this step: `i94_port.csv`\n",
    "- Cleaning task to do: \n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "    - Clean leading and trailing white space.\n",
    "    - Split to port_code, city, state.\n",
    "    - Clean others columns to limit dataset.\n",
    "    - Create and verify staging table.\n",
    "- Jupiter Notebook for cleaning\n",
    "    ```code reference\n",
    "        Extract_I94_SAS_Labels-v03d.ipynb\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As expectation mention, we want to find out the relations between US immigration with either weather, immigration traffic and the arrival place (city). To archive the expectation, we create star data modeling with fact and dim tables detail bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fact table:\n",
    "- The fact table `fact_i94immi` should includes columns:\n",
    "    - `traveller_cicid`\n",
    "    - `arr_airport_code`\n",
    "    - `arr_city`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `arr_datetime_iso`\n",
    "    - `arr_year`\n",
    "    - `arr_month`\n",
    "    - `arr_state_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dimension tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` \n",
    "    - `immi_datetime_iso`\n",
    "    - `arr_port_code`\n",
    "    - `travel_city`\n",
    "    - `travel_month`\n",
    "    - `travel_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `dim_i94immi_airline` airline, flight number, flight time. (not yet)\n",
    "    - `immi_cicid` \n",
    "    - `immi_datetime_iso`\n",
    "    - `arr_port_code`\n",
    "    - `travel_city`\n",
    "    - `travel_month`\n",
    "    - `travel_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `fact_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect`\n",
    "    - `avg_tempe`\n",
    "    - `avg_uncertain_tempe`\n",
    "    - `tempe_month`\n",
    "    - `tempe_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `dim_us_tempetime` contains time event of temperature collected\n",
    "    - `tempe_datetime`\n",
    "    - `tempe_month`\n",
    "    - `tempe_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code`\n",
    "    - `city_name`\n",
    "    - `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year`\n",
    "    - `arrival_month`\n",
    "    - `arrival_date`\n",
    "    - dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw dataset from source into dataframes:\n",
    "    - I94 Immigration to `i94immi_df` as Spark dataframe.\n",
    "    - WWorld Temperature to `worldtempe_df` as Pandas dataframe.\n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS` as a Spark dataframe.\n",
    "- Describe and Gather Data on:\n",
    "    - `i94immi_df` as Spark dataframe.\n",
    "    - `worldtempe_df` as Pandas dataframe.\n",
    "    - `I94PORT` as a Spark dataframe.\n",
    "- Clean each Spark dataframe as decscibed to staging dataset:\n",
    "    - `i94immi_df` cleaned output to `i94immi_df_clean` as a csv format.\n",
    "    - `worldtempe_df` cleaned output to `worldtempe_df_clean` as a csv format.\n",
    "    - `I94PORT` cleaned output to `i94port_staging` as a csv format.\n",
    "- Transform csv staging datasets to staging tables:\n",
    "    - `i94immi_df_clean` staging to `i94immi_table` as Spark SQL table.\n",
    "    - `worldtempe_df_clean` cleaned output to `worldtempe_table` as Spark SQL table.\n",
    "    - `i94port_staging` cleaned output to `i94port_table` as Spark SQL table.\n",
    "- Transform staging tables to fact and dim tables.\n",
    "    - `dim_immi_travller` is transformed from `i94immi_table`, `dim_port`.\n",
    "    - `dim_us_temperature` is transformed from `worldtempe_table` and `dim_datetime`.\n",
    "    - `dim_datetime` is transformed from `i94immi_table`.\n",
    "    - `dim_port` is transformed from `i94port_table`.\n",
    "    - `fact_immi_weather` is loaded from dim tables.\n",
    "- Create and run pipeline to model data.\n",
    "    - Load raw datasets; Describe and Gather Data: `Describe_and_Gather_Data-submit-03b.ipynb`\n",
    "    - Clean and staging datasets:\n",
    "        - `Extract_I94_SAS_Labels-v03d.ipynb`\n",
    "        - `cleaning_staging_i94immi-v3c.ipynb`\n",
    "        - `cleaning_staging_worldtempe-v3c.ipynb`\n",
    "    - Transform to fact and dim tables:\n",
    "        - `transform_fact_dims-v03c.ipynb`\n",
    "- Create data quality check for fact and dim tables.\n",
    "    - For dim tables `quality_check_dims.ipynb` (not yet)\n",
    "    - For fact tables `quality_check_fact.ipynb` (not yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.1 Create the data model\n",
    "Run steps of pipeline\n",
    "\n",
    "- `Describe_and_Gather_Data-submit-03b.ipynb`\n",
    "- `Extract_I94_SAS_Labels-v03d.ipynb`\n",
    "- `cleaning_staging_i94immi-v3c.ipynb`\n",
    "- `cleaning_staging_worldtempe-v3c.ipynb`\n",
    "- `transform_fact_dims-v03c.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The fact table `fact_immi_weather` should includes columns:\n",
    "    - `traveller_cicid` as `varchar` datatype\n",
    "    - `arr_airport_code` as `varchar` datatype\n",
    "    - `arr_city` as `varchar` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `arr_datetime_iso` as `datetime` datatype\n",
    "    - `arr_year` as `datetime` datatype\n",
    "    - `arr_month` as `datetime` datatype\n",
    "    - `arr_state_code` as `varchar` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dim tables\n",
    "\n",
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` as `varchar` datatype\n",
    "    - `immi_datetime_iso` as `datetime` datatype\n",
    "    - `arr_port_code` as `varchar` datatype\n",
    "    - `travel_city` as `varchar` datatype\n",
    "    - `travel_month` as `datetime` datatype\n",
    "    - `travel_year` as `datetime` datatype\n",
    "\n",
    "- `dim_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect` as `datetime` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `tempe_month` as `datetime` datatype\n",
    "    - `tempe_year` as `datetime` datatype\n",
    "\n",
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code` as `varchar` datatype\n",
    "    - `city_name` as `varchar` datatype\n",
    "    - `state` as `varchar` datatype\n",
    "\n",
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year` as `datetime` datatype\n",
    "    - `arrival_month` as `datetime` datatype\n",
    "    - `arrival_date` as `datetime` datatype\n",
    "    * dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - Pandas was chosen since it can easily handle dataframe either input or output csv. Easy to install and config Pandas on local computer to push up progress with SDK as VScode or Atom.\n",
    "    - Spark were chosen since capable of handling support file formats (ex. sas7bdat, SAS) with large volume of data.\n",
    "    - Dataframes of Spark or Pandas can be converted to each other.\n",
    "    - Spark SQL was chosen since capable of processing the large input files into dataframes and manipulated via commom SQL JOIN, modify table structure, aggregate data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    - Depending on the purpose of use to make period data analysis suggestions.\n",
    "        * Yearly: To have data for data analysis education\n",
    "        * Monthly: To have a basis for predicting seasonal travller traffic.\n",
    "        * Weekly: To have a basis for serving medical purposes (tracing infectious diseases like COVID, flu, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios the data was increased by 100x:\n",
    "    - For the case that need to be met in terms of periodic timing, 100x is a really big volume to meet target be on time. For this scenarios, we should make design a region distributed big data for collecting, processing, modeling. This distributed big data should be many cluster on cloud-based system. Data will be split to chunks, every chunks being processed by a batch-job.\n",
    "    - For the case no worry about time, we can process the data as a single batch job. We could use existing cloud-based bigdata processing solution as datalake, datawarehouse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For specific time update dashboard visualization, we can apply pipeline automation. Airflow is a good candidate cause of capacities schedule, automation, processing batch-job.\n",
    "    - The configurations of cloud-based processing system is a key factor to speed up the analysis progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    - We could consider publishing the parquet files to read-ony HDFS. \n",
    "    - In scenarios need  to meet high frequency SQL queries we consider place a midleware (ex. redis, memcahe, memsql) layer in the front of database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
