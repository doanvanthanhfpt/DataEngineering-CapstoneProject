{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "#### Project Description\n",
    "The target of project is analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "- `I94 Immigration` The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "- `World Temperature Data` This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "- `I94_SAS_Labels_Descriptions.SAS` to get validation dataset. We will use `I94Port.txt` as list of airport, city, state.\n",
    "\n",
    "#### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* `I94 Immigration`\n",
    "* `World Temperature Data`\n",
    "* `I94_SAS_Labels_Descriptions.SAS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technologies will be used:\n",
    "- Spark, Spark SQL\n",
    "- Python, Pandas\n",
    "- AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration dataset:\n",
    "    - `cicid`: Visitor US cid code issue on every travller get throught the immigration port.\n",
    "    - `i94yr | i94mon`: Year, Month of immigration date.\n",
    "    - `i94cit | i94res`: Country of Citizenship & Country of recidence. From `I94_SAS_Labels_Descriptions.SAS`. Don't use.\n",
    "    - `i94port`: Port code for a specific immigration port USA city. From `I94_SAS_Labels_Descriptions.SAS`. There are types of airport that do not allow immigration entry.\n",
    "    - `arrdate | depdate`: Arrival date in the USA & Departure date from the USA.\n",
    "    - `i94mode`: Code for immigration transportation mode. From `I94_SAS_Labels_Descriptions.SAS`. There are methods of immigration transportation without airport gateway. Don't use.\n",
    "    - `i94addr`: US state code. From `I94_SAS_Labels_Descriptions.SAS`.\n",
    "    - `i94bir`: Age of traveller in Years. Don't use.\n",
    "    - `i94visa`: Code for visa type corresponse to visiting reason. Don't use.\n",
    "    - `count, tadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, insnum`: Don't use.\n",
    "    - `biryear`: Immigrant year of birth. Have to review data type. Don't use.\n",
    "    - `gender`: Immigrant sex. There are some un-common sex kind. Don't use.\n",
    "    - `airline`: Airline Coporate used to arrive in U.S. Don't use.\n",
    "    - `admnum`: Admission Number. Don't use.\n",
    "    - `fltno`: Flight number of Airline used to arrive in U.S. Don't use.\n",
    "    - `visatype`: Class of admission legally admitting the non-immigrant to temporarily stay in U.S. Don't use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature dataset\n",
    "    - `dt`: The creation time of temperature.\n",
    "    - `AverageTemperature | AverageTemperatureUncertainty`: temperature value recognized.\n",
    "    - `City | Country`: City of Country that the teperature recognized.\n",
    "    - `Latitude | Longitude`: Geographical location in lat-long. Helpful for heatmap but these columns is useless in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94_SAS_Labels_Descriptions.SAS contains information parts:\n",
    "    - `I94PORT_sas_label_validation` (to be used later)\n",
    "        - `i94port_valid_code`: airport code.\n",
    "        - `i94port_city_name`: the city corresponding to airport code.\n",
    "        - `i94port_state_code`: the state the city belong to.\n",
    "    - `I94MODE_sas_label_validation`\n",
    "        - `i94mode_valid_code`: Immigration mode code\n",
    "        - `i94mode_valid_value`: Immigration mode value\n",
    "    - `I94VISA_sas_label_validation`\n",
    "        - `i94visa_valid_code`: Code of visa type of travaller\n",
    "        - `i94visa_valid_value`: Value of visa type of travller\n",
    "    - `I94ADDR_sas_label_validation`\n",
    "        - `i94addr_valid_code`: Immigration state code\n",
    "        - `i94addr_valid_value`: Immigration state value\n",
    "    - `I94RES_sas_label_validation`\n",
    "        - `i94res_valid_code`: The country code that traveller come from\n",
    "        - `i94res_valid_value`: The country value that traveller come from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perform outputs:\n",
    "- Amount of records and data size: \n",
    "    - I94 Immigration Dataset: `3096313 rows`\n",
    "    - World Temperature Dataset: `8599212 rows`\n",
    "    - i94port SAS Labels Dataset: `660 rows`\n",
    "- Data file extension formats included: \n",
    "    - I94 Immigration Dataset is a `.sas7bdat`\n",
    "    - World Temperature Dataset is a `.csv`\n",
    "    - SAS Labels Descriptions is a `.SAS`\n",
    "\n",
    "Our choosen datesets sastify the project rubric and will be using for data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectations :\n",
    "- The choosen datasets enough to perform a data modeling of fact and dimention tables to analysis the relationship between amount of travel immigration and weather duration by month of city.\n",
    "- Visa to city\n",
    "- Airline traffic\n",
    "- Immigration age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.core.multiarray' has no attribute 'from_dlpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Do all imports and installs here - Done\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, udf\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\__init__.py:54\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cast, Any, Callable, Optional, TypeVar, Union\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatusTracker, SparkJobInfo, SparkStageInfo\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py:75\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     AutoBatchedSerializer,\n\u001b[0;32m     57\u001b[0m     BatchedSerializer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     write_int,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjoin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     python_join,\n\u001b[0;32m     70\u001b[0m     python_left_outer_join,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     python_cogroup,\n\u001b[0;32m     74\u001b[0m )\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatcounter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatCounter\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrddsampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDDSampler, RDDRangeSampler, RDDStratifiedSampler\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstoragelevel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageLevel\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\statcounter.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Iterable, Optional\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maximum, minimum, sqrt\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     maximum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py:140\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py:99\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumeric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mabs\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# do this after everything else, to minimize the chance of this misleadingly\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# appearing in an import-time traceback\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_newdocs\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_newdocs_scalars\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_add_newdocs.py:1598\u001b[0m\n\u001b[0;32m   1447\u001b[0m add_newdoc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy.core.multiarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfromfile\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1448\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;124;03m    fromfile(file, dtype=float, count=-1, sep='', offset=0, *, like=None)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1539\u001b[0m         array_function_like_doc,\n\u001b[0;32m   1540\u001b[0m     ))\n\u001b[0;32m   1542\u001b[0m add_newdoc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy.core.multiarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrombuffer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;124;03m    frombuffer(buffer, dtype=float, count=-1, offset=0, *, like=None)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1595\u001b[0m         array_function_like_doc,\n\u001b[0;32m   1596\u001b[0m     ))\n\u001b[1;32m-> 1598\u001b[0m \u001b[43madd_newdoc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy.core.multiarray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrom_dlpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;124;43;03m    from_dlpack(x, /)\u001b[39;49;00m\n\u001b[0;32m   1601\u001b[0m \n\u001b[0;32m   1602\u001b[0m \u001b[38;5;124;43;03m    Create a NumPy array from an object implementing the ``__dlpack__``\u001b[39;49;00m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;124;43;03m    protocol. Generally, the returned NumPy array is a read-only view\u001b[39;49;00m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;124;43;03m    of the input object. See [1]_ and [2]_ for more details.\u001b[39;49;00m\n\u001b[0;32m   1605\u001b[0m \n\u001b[0;32m   1606\u001b[0m \u001b[38;5;124;43;03m    Parameters\u001b[39;49;00m\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;124;43;03m    ----------\u001b[39;49;00m\n\u001b[0;32m   1608\u001b[0m \u001b[38;5;124;43;03m    x : object\u001b[39;49;00m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;124;43;03m        A Python object that implements the ``__dlpack__`` and\u001b[39;49;00m\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;124;43;03m        ``__dlpack_device__`` methods.\u001b[39;49;00m\n\u001b[0;32m   1611\u001b[0m \n\u001b[0;32m   1612\u001b[0m \u001b[38;5;124;43;03m    Returns\u001b[39;49;00m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;124;43;03m    -------\u001b[39;49;00m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;43;03m    out : ndarray\u001b[39;49;00m\n\u001b[0;32m   1615\u001b[0m \n\u001b[0;32m   1616\u001b[0m \u001b[38;5;124;43;03m    References\u001b[39;49;00m\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;124;43;03m    ----------\u001b[39;49;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;124;43;03m    .. [1] Array API documentation,\u001b[39;49;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;124;43;03m       https://data-apis.org/array-api/latest/design_topics/data_interchange.html#syntax-for-data-interchange-with-dlpack\u001b[39;49;00m\n\u001b[0;32m   1620\u001b[0m \n\u001b[0;32m   1621\u001b[0m \u001b[38;5;124;43;03m    .. [2] Python specification for DLPack,\u001b[39;49;00m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;124;43;03m       https://dmlc.github.io/dlpack/latest/python_spec.html\u001b[39;49;00m\n\u001b[0;32m   1623\u001b[0m \n\u001b[0;32m   1624\u001b[0m \u001b[38;5;124;43;03m    Examples\u001b[39;49;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;124;43;03m    --------\u001b[39;49;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;124;43;03m    >>> import torch\u001b[39;49;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;124;43;03m    >>> x = torch.arange(10)\u001b[39;49;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;124;43;03m    >>> # create a view of the torch tensor \"x\" in NumPy\u001b[39;49;00m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;43;03m    >>> y = np.from_dlpack(x)\u001b[39;49;00m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1632\u001b[0m add_newdoc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy.core\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastCopyAndTranspose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;124;03m\"\"\"_fastCopyAndTranspose(a)\"\"\"\u001b[39;00m)\n\u001b[0;32m   1635\u001b[0m add_newdoc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy.core.multiarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;124;03m\"\"\"cross_correlate(a,v, mode=0)\"\"\"\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py:521\u001b[0m, in \u001b[0;36madd_newdoc\u001b[1;34m(place, obj, doc, warn_on_python)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_newdoc\u001b[39m(place, obj, doc, warn_on_python\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03m    Add documentation to an existing object, typically one defined in C\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    If possible it should be avoided.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    523\u001b[0m         _add_docstring(new, doc\u001b[38;5;241m.\u001b[39mstrip(), warn_on_python)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy.core.multiarray' has no attribute 'from_dlpack'"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "            config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "            config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "            config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\").\\\n",
    "            config(\"fs.s3a.aws.credentials.provider\",\"com.amazonaws.auth.InstanceProfileCredentialsProvider,com.amazonaws.auth.DefaultAWSCredentialsProviderChain\").\\\n",
    "            config(\"fs.AbstractFileSystem.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3A\").\\\n",
    "            enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure read out validation pair values from SAS Labels Description\n",
    "def get_validation_code_from_SAS_labels(sas_input_label):\n",
    "    '''\n",
    "    This procedure read a input SAS Labels Description and then write out validation code datasets.\n",
    "    The SAS Labels Description included validation code datasets with labels: I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sas_input_label : string\n",
    "        The label name of validation code dataset. Its can be one of I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    validation_code_list : validation_value_pairs(tuple(str_valid_code, str_valid_value))\n",
    "        The return output is a specific SAS label list of validation code value pairs.\n",
    "    '''\n",
    "\n",
    "    # Read input SAS Labels Descriptions\n",
    "    with open('I94_SAS_Labels_Descriptions.SAS') as sas_validation_code:\n",
    "            labels_from_sas = sas_validation_code.read()\n",
    "\n",
    "    # Parse labels from SAS Label Description input\n",
    "    sas_labels = labels_from_sas[labels_from_sas.index(sas_input_label):]\n",
    "    sas_labels = sas_labels[:sas_labels.index(';')]\n",
    "    \n",
    "    # Processing line by line, remove separate charaters and then append value pair\n",
    "    lines = sas_labels.splitlines()\n",
    "    validation_code_list = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            valid_code, valid_value = line.split('=')\n",
    "            valid_code = valid_code.strip().strip(\"'\").strip('\"')\n",
    "            valid_value = valid_value.strip().strip(\"'\").strip('\"').strip()\n",
    "            validation_code_list.append((valid_code, valid_value))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return validation_code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure extract parts from SAS Labels Description\n",
    "def extract_staging_sas_label(label):\n",
    "    '''\n",
    "    This procedure get a specific part of data dictionary from SAS_Labels_Descriptions.SAS to a schema.\n",
    "    The dictionary part include valid_code and valid_value\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    label: \n",
    "        a string input of specific label from \"SAS_Label_Descriptions.SAS\"\n",
    "        \n",
    "    Syntax note: \n",
    "        input value in string datatype, need inside a pair of single quotes. Ex: 'I94RES', 'I94PORTS'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dir of csv files with a specific part as input label.\n",
    "    '''\n",
    "    label_name = label\n",
    "    valid_code = label + \"_valid_code\"\n",
    "    valid_value = label + \"_valid_value\"\n",
    "    csv_output = label + \"_sas_label_validation\"\n",
    "    \n",
    "    # Create dir for output\n",
    "    parent_dir = \"./\"\n",
    "    path = os.path.join(parent_dir, csv_output)\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" % csv_output)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\" % csv_output)\n",
    "\n",
    "    # Define output dataframe structure\n",
    "    schema = R([\n",
    "        Fld(valid_code, Str()),\n",
    "        Fld(valid_value, Str())\n",
    "    ])\n",
    "\n",
    "    # Create dataframe from extracted label\n",
    "    df = spark.createDataFrame(\n",
    "        data=get_validation_code_from_SAS_labels(label_name),\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # df.write.mode('overwrite').csv(csv_output)\n",
    "    shutil.rmtree(csv_output, ignore_errors=False, onerror=None)\n",
    "    df.write.options(header='True', delimiter=',').csv(csv_output)\n",
    "\n",
    "    df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(csv_output)\n",
    "\n",
    "    print(\"Top 20 rows of {} \".format(csv_output))\n",
    "    df.show()\n",
    "\n",
    "    print(\"Count rows of {}: {} \".format(csv_output, df.count()))\n",
    "    \n",
    "    print(\"Check unique value of {}: {} \".format(csv_output, df.select(valid_code).distinct().count()))\n",
    "\n",
    "    print(\"Staging csv files in: {}\".format(csv_output))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure convert column name\n",
    "def convert_column_names(df):\n",
    "    '''\n",
    "    This procedure standardizing column names to snake case format. Format ex: customer_name, billing_address, total_price.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : string_of_dataframe\n",
    "        The input dataframe with column names might have elements of messy columns names, including accents, different delimiters, casing and multiple white spaces.\n",
    "        Snake case style replaces the white spaces and symbol delimiters with underscore and converts all characters to lower case\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with column names has been changed to snake_case format.\n",
    "    '''\n",
    "    cols = df.columns\n",
    "    column_name_changed = []\n",
    "\n",
    "    for col in cols:\n",
    "        new_column = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")\n",
    "        column_name_changed.append(new_column)\n",
    "\n",
    "    df.columns = column_name_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure remove specific dir (if need)\n",
    "def rmdir(directory):\n",
    "    '''\n",
    "    This procedure perform pure recursive a directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string_of_path_to_dir\n",
    "        The input directory is a path to target dir. This dir and all its belong child objects wil be deleted.\n",
    "        Syntax note: rmdir(Path(\"target_path_to_dir\"))\n",
    "            with Path(\"target_path_to_dir\") returns path to dir format as 'directory' input\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    directory = Path(directory)\n",
    "    for item in directory.iterdir():\n",
    "        if item.is_dir():\n",
    "            rmdir(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "    directory.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input dataset to Spark dataframe\n",
    "# i94immi_dataset = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# i94_immi_df = spark.read.format('com.github.saurfang.sas.spark').load(i94immi_dataset)\n",
    "i94immi_df = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type\n",
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes columns\n",
    "i94immi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94immi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### World Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input dataset to Pandas dataframe\n",
    "worldtempe_dataset = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "worldtempe_df = pd.read_csv(worldtempe_dataset,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type\n",
    "worldtempe_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes columns\n",
    "worldtempe_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldtempe_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94PORT_sas_label_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'I94PORT_sas_label_validation' created successfully\n",
      "Top 20 rows of I94PORT_sas_label_validation \n",
      "+------------------+--------------------+\n",
      "|I94PORT_valid_code| I94PORT_valid_value|\n",
      "+------------------+--------------------+\n",
      "|               ALC|           ALCAN, AK|\n",
      "|               ANC|       ANCHORAGE, AK|\n",
      "|               BAR|BAKER AAF - BAKER...|\n",
      "|               DAC|   DALTONS CACHE, AK|\n",
      "|               PIZ|DEW STATION PT LA...|\n",
      "|               DTH|    DUTCH HARBOR, AK|\n",
      "|               EGL|           EAGLE, AK|\n",
      "|               FRB|       FAIRBANKS, AK|\n",
      "|               HOM|           HOMER, AK|\n",
      "|               HYD|           HYDER, AK|\n",
      "|               JUN|          JUNEAU, AK|\n",
      "|               5KE|       KETCHIKAN, AK|\n",
      "|               KET|       KETCHIKAN, AK|\n",
      "|               MOS|MOSES POINT INTER...|\n",
      "|               NIK|         NIKISKI, AK|\n",
      "|               NOM|             NOM, AK|\n",
      "|               PKC|     POKER CREEK, AK|\n",
      "|               ORI|  PORT LIONS SPB, AK|\n",
      "|               SKA|         SKAGWAY, AK|\n",
      "|               SNP| ST. PAUL ISLAND, AK|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Count rows of I94PORT_sas_label_validation: 660 \n",
      "Check unique value of I94PORT_sas_label_validation: 660 \n",
      "Staging csv files in: I94PORT_sas_label_validation\n"
     ]
    }
   ],
   "source": [
    "# Extract `I94PORT` label\n",
    "# Create dir 'I94PORT_sas_label_validation' to save extracted label result\n",
    "I94PORT_df = extract_staging_sas_label('I94PORT')\n",
    "I94PORT_df = I94PORT_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type\n",
    "I94PORT_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes columns\n",
    "I94PORT_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows\n",
    "I94PORT_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94ADDR_sas_label_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'I94ADDR_sas_label_validation' created successfully\n",
      "Top 20 rows of I94ADDR_sas_label_validation \n",
      "+------------------+-------------------+\n",
      "|I94ADDR_valid_code|I94ADDR_valid_value|\n",
      "+------------------+-------------------+\n",
      "|                AL|            ALABAMA|\n",
      "|                AK|             ALASKA|\n",
      "|                AZ|            ARIZONA|\n",
      "|                AR|           ARKANSAS|\n",
      "|                CA|         CALIFORNIA|\n",
      "|                CO|           COLORADO|\n",
      "|                CT|        CONNECTICUT|\n",
      "|                DE|           DELAWARE|\n",
      "|                DC|  DIST. OF COLUMBIA|\n",
      "|                FL|            FLORIDA|\n",
      "|                GA|            GEORGIA|\n",
      "|                GU|               GUAM|\n",
      "|                HI|             HAWAII|\n",
      "|                ID|              IDAHO|\n",
      "|                IL|           ILLINOIS|\n",
      "|                IN|            INDIANA|\n",
      "|                IA|               IOWA|\n",
      "|                KS|             KANSAS|\n",
      "|                KY|           KENTUCKY|\n",
      "|                LA|          LOUISIANA|\n",
      "+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Count rows of I94ADDR_sas_label_validation: 55 \n",
      "Check unique value of I94ADDR_sas_label_validation: 55 \n",
      "Staging csv files in: I94ADDR_sas_label_validation\n"
     ]
    }
   ],
   "source": [
    "# Extract `I94ADDR` label\n",
    "# Create dir 'I94ADDR_sas_label_validation' first to save extracted label result\n",
    "I94ADDR_df = extract_staging_sas_label('I94ADDR')\n",
    "I94ADDR_df = I94ADDR_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type\n",
    "I94ADDR_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes columns\n",
    "I94ADDR_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows\n",
    "I94ADDR_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning I94 Immigration dataset of a month `../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat`\n",
    "- The input dataframe from Explorer step `i94immi_df = spark.read.parquet(\"sas_data\")`\n",
    "- The outputs of this step: `i94immi_df_clean.csv`\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to spark dataframe.\n",
    "    - Create Spark SQL table from dataframe.\n",
    "    - Choose Primarykey.\n",
    "    - Verify arrival date and departure date logical conditional.\n",
    "    - Add column `arival_date`, `departure_date` as `datetime` datatype\n",
    "    - Verify `arival_date`, `departure_date` wrong value.\n",
    "    - Filter US airport with immigration allowed.\n",
    "    - Remove missing value.\n",
    "    - Create and verify staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tableview for data manipulation\n",
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop amount of un-makesance records cause by `DepartureDate >= ArrivalDate`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE arrdate <= depdate\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column `arrival_date = timestone + arrdate_offset_day`, with:\n",
    "# - timestone = '1960-01-01' (***datetime*** datatype)\n",
    "# - arrdate_offset_day = 'arrdate' (***integer*** datatype)\n",
    "# - arrival_date (***datetime*** datatype)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date \n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column `departure_date = timestone + depdate_offset_day`, with:\n",
    "# - `timestone` = '1960-01-01' (***datetime*** datatype)\n",
    "# - `depdate_offset_day` = 'depdate' (***integer*** datatype)\n",
    "# - `departure_date` (***datetime*** datatype)\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= arrdate THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'NaN' END AS departure_date \n",
    "                FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted i94mode from `I94_SAS_Labels_Descriptions_SAS`\n",
    "# i94mode includes:\n",
    "# {'1': 'Air', '2': 'Sea', '3': 'Land', '9': 'Not reported'}\n",
    "# Keep air arrival only, mean keep `i94mode=1`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94mode == 1.0\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping `i94visa` numbers to `visatype` instead\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *, CASE \n",
    "                    WHEN i94visa = 1.0 THEN 'Business' \n",
    "                    WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                    WHEN i94visa = 3.0 THEN 'Student'\n",
    "                    ELSE 'NaN' END AS visa_type\n",
    "        FROM i94immi_table\n",
    "    \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep user records of `male = 'M'` and `female = 'F'` only\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM i94immi_table \n",
    "    WHERE gender IN ('F', 'M')\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NULL value on arrival state\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94addr IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns\n",
    "# Convert month and year timestamp\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cicid,\n",
    "            i94cit,\n",
    "            i94res,\n",
    "            i94port,\n",
    "            arrival_date,\n",
    "            YEAR(arrival_date) as i94yr,\n",
    "            MONTH(arrival_date) as i94mon,\n",
    "            i94mode,\n",
    "            i94addr,\n",
    "            departure_date,\n",
    "            i94bir,\n",
    "            i94visa,\n",
    "            count,\n",
    "            dtadfile,\n",
    "            biryear,\n",
    "            dtaddto,\n",
    "            gender,\n",
    "            insnum,\n",
    "            airline,\n",
    "            admnum,\n",
    "            fltno,\n",
    "            visatype,\n",
    "            visa_type\n",
    "        FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the cleaned\n",
    "i94immi_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|arrival_date|i94yr|i94mon|i94mode|i94addr|departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     CA|    2016-05-08|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     NV|    2016-05-17|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     WA|    2016-05-08|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|            2377896|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleaned\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94immi_rows\n",
    "    FROM i94immi_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './i94immi_df_clean' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Staging to csv file\n",
    "path = './i94immi_df_clean'\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "\n",
    "rmdir(Path(\"i94immi_df_clean\")) # use this line from 2nd running\n",
    "i94immi_df.write.options(header='True', delimiter=',').csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify out from staging csv partitions\n",
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|       arrival_date|i94yr|i94mon|i94mode|i94addr|     departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     CA|2016-05-08 00:00:00|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     NV|2016-05-17 00:00:00|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-08 00:00:00|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare to save out s3 storage\n",
    "i94immi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'double'),\n",
       " ('i94cit', 'double'),\n",
       " ('i94res', 'double'),\n",
       " ('i94port', 'string'),\n",
       " ('arrival_date', 'timestamp'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94mode', 'double'),\n",
       " ('i94addr', 'string'),\n",
       " ('departure_date', 'timestamp'),\n",
       " ('i94bir', 'double'),\n",
       " ('i94visa', 'double'),\n",
       " ('count', 'double'),\n",
       " ('dtadfile', 'int'),\n",
       " ('biryear', 'double'),\n",
       " ('dtaddto', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'double'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string'),\n",
       " ('visa_type', 'string')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### World Temperature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning World Temperature dataset `../../data2/GlobalLandTemperaturesByCity.csv`\n",
    "- The input dataframe from Explorer step: `worldtempe_df = pd.read_csv(worldtempe_dataset,sep=\",\")`\n",
    "- The outputs of this step: `worldtempe_df_clean.csv`.\n",
    "- Cleaning task to do:\n",
    "    - Read dataset to pandas dataframe.\n",
    "    - Filter value of `United States` only.\n",
    "    - Limit dataset duration by immigration time duration\n",
    "    - Clean column with datetime datatype.\n",
    "    - Standalizing column names format.\n",
    "    - Create and verify staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out `Country` for single value `United States` and check dataframe size\n",
    "worldtempe_df = worldtempe_df[worldtempe_df['Country']=='United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'dt_converted' as datetime datatype\n",
    "worldtempe_df['dt_converted'] = pd.to_datetime(worldtempe_df.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut off the sub-dataset before \"1960-01-01\"\n",
    "worldtempe_df=worldtempe_df[worldtempe_df['dt_converted']>\"1960-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper case 'City' column values\n",
    "worldtempe_df[\"City\"] = worldtempe_df[\"City\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dt', 'averagetemperature', 'averagetemperatureuncertainty', 'city',\n",
       "       'country', 'latitude', 'longitude', 'dt_converted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert column names to ***snake_case*** format. Format ex: *customer_name, billing_address, ...*\n",
    "convert_column_names(worldtempe_df)\n",
    "worldtempe_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging to csv file (optional)\n",
    "worldtempe_df.to_csv('worldtempe_df_clean.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify out from staging csv partitions (optional)\n",
    "# worldtempe_df = pd.read_csv('worldtempe_df_clean.csv',sep=\",\")\n",
    "worldtempe_df = spark.read.csv(\"worldtempe_df_clean.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'string'),\n",
       " ('averagetemperature', 'string'),\n",
       " ('averagetemperatureuncertainty', 'string'),\n",
       " ('city', 'string'),\n",
       " ('country', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('dt_converted', 'string')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worldtempe_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+------------+\n",
      "|        dt|averagetemperature|averagetemperatureuncertainty|   city|      country|latitude|longitude|dt_converted|\n",
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+------------+\n",
      "|1960-02-01|             4.995|                        0.325|ABILENE|United States|  32.95N|  100.53W|  1960-02-01|\n",
      "|1960-03-01| 8.575000000000001|                        0.303|ABILENE|United States|  32.95N|  100.53W|  1960-03-01|\n",
      "|1960-04-01|            18.452|                        0.282|ABILENE|United States|  32.95N|  100.53W|  1960-04-01|\n",
      "|1960-05-01|            21.709|          0.28600000000000003|ABILENE|United States|  32.95N|  100.53W|  1960-05-01|\n",
      "|1960-06-01|            27.714|                        0.387|ABILENE|United States|  32.95N|  100.53W|  1960-06-01|\n",
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worldtempe_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatype of column '' as DoubleType datatype for average temperature of eache city\n",
    "worldtempe_df = worldtempe_df.withColumn(\"averagetemperature\", worldtempe_df[\"averagetemperature\"].cast(DoubleType()).alias(\"averagetemperature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lamda function to convert string of date format to date datatype\n",
    "func =  udf (lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'dt_converted' to date datatype\n",
    "worldtempe_df = worldtempe_df.withColumn('dt_converted', func(col('dt_converted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tableview for Spark SQL dataframe manipulation\n",
    "worldtempe_df.createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns 'tempe_month' as date and 'tempe_year' as date\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            dt_converted,\n",
    "            MONTH(worldtempe_table.dt_converted) as tempe_month,\n",
    "            YEAR(worldtempe_table.dt_converted) as tempe_year,\n",
    "            dt,\n",
    "            city,\n",
    "            averagetemperature,\n",
    "            averagetemperatureuncertainty\n",
    "        FROM worldtempe_table\n",
    "            \"\"\").createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping average temperature by city for city temperature consistent\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        tempe_month,\n",
    "        BROUND(AVG(averagetemperature),2) as averagetemperature,\n",
    "        BROUND(AVG(averagetemperatureuncertainty),2) as averagetemperatureuncertainty,\n",
    "        tempe_year,\n",
    "        dt_converted\n",
    "    FROM worldtempe_table\n",
    "    GROUP BY city, tempe_month, tempe_year, dt_converted\n",
    "\"\"\").createOrReplaceTempView('worldtempe_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldtempe_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM worldtempe_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './worldtempe_df_clean' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Staging to csv file\n",
    "path = './worldtempe_df_clean'\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "    \n",
    "rmdir(Path(\"worldtempe_df_clean\")) # use this line from 2nd running\n",
    "worldtempe_df.write.options(header='True', delimiter=',').csv(\"worldtempe_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify out from staging csv partitions\n",
    "worldtempe_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"worldtempe_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|   city|tempe_month|averagetemperature|averagetemperatureuncertainty|tempe_year|       dt_converted|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|ABILENE|         11|              9.77|                         0.12|      1997|1997-11-01 00:00:00|\n",
      "|ABILENE|         10|             18.48|                         0.26|      2000|2000-10-01 00:00:00|\n",
      "|  AKRON|          2|              2.17|                         0.18|      1976|1976-02-01 00:00:00|\n",
      "|  AKRON|          9|              17.5|                         0.22|      1990|1990-09-01 00:00:00|\n",
      "|  AKRON|          7|             24.95|                         0.21|      2011|2011-07-01 00:00:00|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify csv staging\n",
    "worldtempe_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'),\n",
       " ('tempe_month', 'int'),\n",
       " ('averagetemperature', 'double'),\n",
       " ('averagetemperatureuncertainty', 'double'),\n",
       " ('tempe_year', 'int'),\n",
       " ('dt_converted', 'timestamp')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify csv staging\n",
    "worldtempe_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94PORT_sas_label_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning `i94port` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "- The input dataframe from Explorer step: `I94PORT_df = I94PORT_df.toPandas()`\n",
    "- The outputs of this step: `i94_port.csv`\n",
    "- Cleaning task to do: \n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "    - Clean leading and trailing white space.\n",
    "    - Split to port_code, city, state.\n",
    "    - Clean others columns to limit dataset.\n",
    "    - Create and verify staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean leading and trailing white space before split column\n",
    "I94PORT_df[\"I94PORT_valid_code\"] = I94PORT_df[\"I94PORT_valid_code\"].str.lstrip().str.rstrip()\n",
    "I94PORT_df[\"I94PORT_valid_value\"] = I94PORT_df[\"I94PORT_valid_value\"].str.lstrip().str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to port, city, state\n",
    "I94PORT_df[\"I94PORT_city_name\"] = I94PORT_df[\"I94PORT_valid_value\"].str.split(\",\").str.get(0)\n",
    "I94PORT_df[\"I94PORT_state_code\"] = I94PORT_df[\"I94PORT_valid_value\"].str.split(\",\").str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean leading and trailing white space after split column\n",
    "I94PORT_df[\"I94PORT_city_name\"] = I94PORT_df[\"I94PORT_city_name\"].str.lstrip().str.rstrip()\n",
    "I94PORT_df[\"I94PORT_state_code\"] = I94PORT_df[\"I94PORT_state_code\"].str.lstrip().str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing value on I94PORT_state_code\n",
    "I94PORT_df = I94PORT_df.dropna(subset = [\"I94PORT_state_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column name to snake case format\n",
    "convert_column_names(I94PORT_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I94PORT_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'i94port_valid_value' column\n",
    "select_cols = ['i94port_valid_code', 'i94port_city_name', 'i94port_state_code']\n",
    "I94PORT_df = I94PORT_df[select_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I94PORT_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to SQL Spark dataframe for staging output saving\n",
    "I94PORT_df = spark.createDataFrame(I94PORT_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './i94port_staging' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Save to csv staging\n",
    "path = './i94port_staging'\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "    \n",
    "rmdir(Path(\"i94port_staging\")) # use from 2nd running\n",
    "I94PORT_df.write.options(header='True', delimiter=',').csv(\"i94port_staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv staging to create dim table\n",
    "I94PORT_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94port_staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|i94port_valid_code|   i94port_city_name|i94port_state_code|\n",
      "+------------------+--------------------+------------------+\n",
      "|               ALC|               ALCAN|                AK|\n",
      "|               ANC|           ANCHORAGE|                AK|\n",
      "|               BAR|BAKER AAF - BAKER...|                AK|\n",
      "+------------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94PORT_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94ADDR_sas_label_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning `i94addr` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "- The input dataframe from Explorer step: `I94ADDR_df = I94ADDR_df.toPandas()`\n",
    "- The outputs of this step: `i94_addr.csv`\n",
    "- Cleaning task to do: \n",
    "    - Extract `I94PORT` from `I94_SAS_Labels_Descriptions.SAS`\n",
    "    - Clean leading and trailing white space.\n",
    "    - Distinct primarykey columns.\n",
    "    - Create and verify staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean leading and trailing white space before split column\n",
    "I94ADDR_df[\"I94ADDR_valid_code\"] = I94ADDR_df[\"I94ADDR_valid_code\"].str.lstrip().str.rstrip()\n",
    "I94ADDR_df[\"I94ADDR_valid_value\"] = I94ADDR_df[\"I94ADDR_valid_value\"].str.lstrip().str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column name to snake case format\n",
    "convert_column_names(I94ADDR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to SQL Spark dataframe for staging output saving\n",
    "I94ADDR_df = spark.createDataFrame(I94ADDR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './i94addr_staging' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Save to csv staging\n",
    "path = './i94addr_staging'\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "    \n",
    "rmdir(Path(\"i94addr_staging\")) # use from 2nd running\n",
    "I94ADDR_df.write.options(header='True', delimiter=',').csv('i94addr_staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv staging to create dim table\n",
    "I94ADDR_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94addr_staging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|i94addr_valid_code|i94addr_valid_value|\n",
      "+------------------+-------------------+\n",
      "|                AL|            ALABAMA|\n",
      "|                AK|             ALASKA|\n",
      "|                AZ|            ARIZONA|\n",
      "+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94ADDR_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expectation mention, we want to find out the relations between US immigration with either weather, immigration traffic and the arrival place (city). To archive the expectation, we create star data modeling with fact and dim tables detail bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94 Immigration fact table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe come from \n",
    "`i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `fact_i94immi` should includes columns:\n",
    "    - `travel_cicid`\n",
    "    - `from_country_code`\n",
    "    - `immi_country_code`\n",
    "    - `arrival_port_code`\n",
    "    - `immi_arrival_date`\n",
    "    - `arrival_year`\n",
    "    - `arrival_month`\n",
    "    - `airline_mode_code`\n",
    "    - `immi_state_code`\n",
    "    - `departure_date`\n",
    "    - `traveller_age`\n",
    "    - `visatype_by_number`\n",
    "    - `traveller_birth_year`\n",
    "    - `traveller_sex`\n",
    "    - `immi_flight_code`\n",
    "    - `visatype_by_code`\n",
    "    - `visa_type`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|       arrival_date|i94yr|i94mon|i94mode|i94addr|     departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     CA|2016-05-08 00:00:00|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     NV|2016-05-17 00:00:00|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-08 00:00:00|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to Spark SQl\n",
    "i94immi_df.createOrReplaceTempView('fact_i94immi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "fact_i94immi = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            cicid as travel_cicid,\n",
    "            i94cit as from_country_code,\n",
    "            i94res as immi_country_code,\n",
    "            i94port as arrival_port_code,\n",
    "            arrival_date as immi_arrival_date,\n",
    "            i94yr as arrival_year,\n",
    "            i94mon as arrival_month,\n",
    "            i94mode as airline_mode_code,\n",
    "            i94addr as immi_state_code,\n",
    "            departure_date,\n",
    "            i94bir as traveller_age,\n",
    "            i94visa as visatype_by_number,\n",
    "            biryear as traveller_birth_year,\n",
    "            gender as traveller_sex,\n",
    "            fltno as immi_flight_code,\n",
    "            visatype as visatype_by_code,\n",
    "            visa_type\n",
    "        FROM fact_i94immi\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://capstone-project-outputs/s3_outputs/fact_i94immi/fact_i94immi.parquet'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o386.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 24 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-ad3bea81d108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfact_i94immi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_parquet_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o386.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 24 more\n"
     ]
    }
   ],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "fact_i94immi.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visa dim table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe come from \n",
    "`i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")`\n",
    "\n",
    "- `dim_visa` should includes columns:\n",
    "    - `visatype_by_number`\n",
    "    - `visatype_by_code`\n",
    "    - `visa_category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark SQL for dataframe manipulation\n",
    "i94immi_df.createOrReplaceTempView('dim_visa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dim table\n",
    "dim_visa = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            i94visa as visatype_by_number,\n",
    "            visatype as visatype_by_code,\n",
    "            visa_type as visa_category\n",
    "        FROM fact_i94immi\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://capstone-project-outputs/s3_outputs/dim_visa/dim_visa.parquet'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'dim_visa/dim_visa.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "dim_visa.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flight dim table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe come from \n",
    "`i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")`\n",
    "\n",
    "- `dim_immi_flight` should includes columns:\n",
    "    - `flight_brand`\n",
    "    - `flight_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark SQL for dataframe manipulation\n",
    "i94immi_df.createOrReplaceTempView('dim_immi_flight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dim table\n",
    "dim_immi_flight = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            airline as flight_brand,\n",
    "            fltno as flight_number\n",
    "        FROM dim_immi_flight\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'dim_immi_flight/dim_immi_flight.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "dim_immi_flight.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Immigration travller dim table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe come from \n",
    "`i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")`\n",
    "\n",
    "- `dim_immi_travaller` should includes columns:\n",
    "    - `traveller_cicid`\n",
    "    - `from_country_code`\n",
    "    - `immi_country_code`\n",
    "    - `arrival_port_code`\n",
    "    - `arrival_date`\n",
    "    - `airline_immi_code`\n",
    "    - `desination_state_code`\n",
    "    - `traveller_age`\n",
    "    - `traveller_birth_year`\n",
    "    - `traveller_sex`\n",
    "    - `visatye_by_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94immi_df.createOrReplaceTempView('dim_immi_travaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immi_travaller = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            cicid as traveller_cicid,\n",
    "            i94cit as from_country_code,\n",
    "            i94res as immi_country_code,\n",
    "            i94port as arrival_port_code,\n",
    "            arrival_date as arrival_date,\n",
    "            i94mode as airline_immi_code,\n",
    "            i94addr as desination_state_code,\n",
    "            i94bir as traveller_age,\n",
    "            biryear as traveller_birth_year,\n",
    "            gender as traveller_sex,\n",
    "            visatype as visatye_by_code\n",
    "        FROM dim_immi_travaller\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'dim_immi_travaller/dim_immi_travaller.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "dim_immi_travaller.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### World Temperature fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe `worldtempe_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"worldtempe_df_clean\")`\n",
    "\n",
    "- `fact_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `dt`\n",
    "    - `averagetemperature as avg_tempe`\n",
    "    - `averagetemperatureuncertainty as avg_tempe_uncertain`\n",
    "    - `city as measure_city`\n",
    "    - `country as measure_country`\n",
    "    - `latitude`\n",
    "    - `longitude`\n",
    "    - `dt_converted as measure_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tableview for data manipulation\n",
    "worldtempe_df.createOrReplaceTempView('fact_worldtempe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|   city|tempe_month|averagetemperature|averagetemperatureuncertainty|tempe_year|       dt_converted|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|ABILENE|         11|              9.77|                         0.12|      1997|1997-11-01 00:00:00|\n",
      "|ABILENE|         10|             18.48|                         0.26|      2000|2000-10-01 00:00:00|\n",
      "|  AKRON|          2|              2.17|                         0.18|      1976|1976-02-01 00:00:00|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM fact_worldtempe\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "fact_worldtempe = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            averagetemperature as avg_tempe,\n",
    "            averagetemperatureuncertainty as avg_tempe_uncertain,\n",
    "            city as measure_city,\n",
    "            tempe_month as tempe_month,\n",
    "            dt_converted as dt_converted,\n",
    "            dt_converted as measure_date\n",
    "        FROM fact_worldtempe\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|   city|tempe_month|averagetemperature|averagetemperatureuncertainty|tempe_year|       dt_converted|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "|ABILENE|         11|              9.77|                         0.12|      1997|1997-11-01 00:00:00|\n",
      "|ABILENE|         10|             18.48|                         0.26|      2000|2000-10-01 00:00:00|\n",
      "|  AKRON|          2|              2.17|                         0.18|      1976|1976-02-01 00:00:00|\n",
      "+-------+-----------+------------------+-----------------------------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM fact_worldtempe\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://capstone-project-outputs/s3_outputs/fact_worldtempe/fact_worldtempe.parquet'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy S3 URI = 's3a://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'fact_worldtempe/fact_worldtempe.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "fact_worldtempe.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94PORT dim table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe `I94PORT_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94port_staging\")`\n",
    "\n",
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code`\n",
    "    - `city_name`\n",
    "    - `state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to SQL Spark tempview to create dim table\n",
    "I94PORT_df.createOrReplaceTempView('dim_i94port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_i94port = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            i94port_valid_code as immi_port_code,\n",
    "            i94port_city_name as immi_city_name,\n",
    "            i94port_state_code as immi_state_code\n",
    "        FROM dim_i94port\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'dim_i94port/dim_i94port.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "dim_i94port.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I94ADDR dim table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataframe `I94ADDR_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94addr_staging\")`\n",
    "\n",
    "- `dim_i94addr` airline, flight number, flight time. (not yet)\n",
    "    - `immi_state_code` \n",
    "    - `immi_state_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to SQL Spark tempview to create dim table\n",
    "I94ADDR_df.createOrReplaceTempView('dim_i94addr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_i94addr = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            i94addr_valid_code as immi_state_code,\n",
    "            i94addr_valid_code as immi_state_name\n",
    "        FROM dim_i94addr\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy S3 URI = 's3://capstone-project-outputs/s3_outputs/'\n",
    "s3_parquet_output = 's3a://capstone-project-outputs/s3_outputs/' + 'dim_i94addr/dim_i94addr.parquet'\n",
    "s3_parquet_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_parquet_output = 's3://capstone-project-outputs/s3_outputs/' + 'fact_i94immi/fact_i94immi.parquet'\n",
    "dim_i94addr.write.mode(\"overwrite\").parquet(s3_parquet_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw datasources:\n",
    "    - `./sas_data` contains I94 Immigration datasource.\n",
    "    - `../../data2/GlobalLandTemperaturesByCity.csv` contains World Temperature datasource.\n",
    "    - `I94_SAS_Labels_Descriptions.SAS` contains dictionary of `I94PORT` `I94ADDR` `I94VISA` `I94MODE` `I94CIT&RES` datasource.\n",
    "- Describe and Gather Data on:\n",
    "    - `i94immi_df` as Spark dataframe.\n",
    "    - `worldtempe_df` as Pandas dataframe.\n",
    "    - `I94PORT_df` `I94ADDR_df` `I94VISA_df` `I94MODE_df` as a Spark dataframes.\n",
    "- Clean and then staging dataframe:\n",
    "    - `i94immi_df` cleaned and staging as a csv format in dir `i94immi_df_clean`.\n",
    "    - `worldtempe_df` cleaned and staging as a csv format in dir `worldtempe_df_clean`.\n",
    "    - `I94PORT` cleaned and staging as a csv format in dir `I94PORT_sas_label_validation`.\n",
    "    - `I94ADDR` cleaned and staging as a csv format in dir `I94ADDR_sas_label_validation`.\n",
    "- Transform staging tables to fact and dim tables.\n",
    "    - `fact_i94immi`\n",
    "    - `dim_visa`\n",
    "    - `fact_worldtempe`\n",
    "    - `dim_immi_flight`\n",
    "    - `dim_immi_travaller`\n",
    "    - `dim_i94port`\n",
    "    - `dim_i94addr`\n",
    "- Create data quality check for fact and dim tables.\n",
    "    - For dim tables `quality_check_dims.ipynb` (not yet)\n",
    "    - For fact tables `quality_check_fact.ipynb` (not yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Create the data model\n",
    "Run steps of pipeline with a python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The fact table `fact_immi_weather` should includes columns:\n",
    "    - `traveller_cicid` as `varchar` datatype\n",
    "    - `arr_airport_code` as `varchar` datatype\n",
    "    - `arr_city` as `varchar` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `arr_datetime_iso` as `datetime` datatype\n",
    "    - `arr_year` as `datetime` datatype\n",
    "    - `arr_month` as `datetime` datatype\n",
    "    - `arr_state_code` as `varchar` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dim tables\n",
    "\n",
    "- `dim_immi_traveller` contains travller informations like cicid, date, airport, city.\n",
    "    - `immi_cicid` as `varchar` datatype\n",
    "    - `immi_datetime_iso` as `datetime` datatype\n",
    "    - `arr_port_code` as `varchar` datatype\n",
    "    - `travel_city` as `varchar` datatype\n",
    "    - `travel_month` as `datetime` datatype\n",
    "    - `travel_year` as `datetime` datatype\n",
    "\n",
    "- `dim_us_temperature` contains temperature records of US cities has been collect corresponse immigration data scope.\n",
    "    - `city_tempe_collect` as `datetime` datatype\n",
    "    - `avg_tempe` as `doudbletype` datatype\n",
    "    - `avg_uncertain_tempe` as `doudbletype` datatype\n",
    "    - `tempe_month` as `datetime` datatype\n",
    "    - `tempe_year` as `datetime` datatype\n",
    "\n",
    "- `dim_port` contains list of airport allow immigration.\n",
    "    - `port_code` as `varchar` datatype\n",
    "    - `city_name` as `varchar` datatype\n",
    "    - `state` as `varchar` datatype\n",
    "\n",
    "- `dim_datetime` contains date information like year, month, day, week of year and weekday.\n",
    "    - `arrival_year` as `datetime` datatype\n",
    "    - `arrival_month` as `datetime` datatype\n",
    "    - `arrival_date` as `datetime` datatype\n",
    "    * dim_datetime created by append datetime from staging data `i94immi_table`. In this project we use **2016 April** only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - Pandas was chosen since it can easily handle dataframe either input or output csv. Easy to install and config Pandas on local computer to push up progress with SDK as VScode or Atom.\n",
    "    - Spark were chosen since capable of handling support file formats (ex. sas7bdat, SAS) with large volume of data.\n",
    "    - Dataframes of Spark or Pandas can be converted to each other.\n",
    "    - Spark SQL was chosen since capable of processing the large input files into dataframes and manipulated via commom SQL JOIN, modify table structure, aggregate data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    - Depending on the purpose of use to make period data analysis suggestions.\n",
    "        * Yearly: To have data for data analysis education\n",
    "        * Monthly: To have a basis for predicting seasonal travller traffic.\n",
    "        * Weekly: To have a basis for serving medical purposes (tracing infectious diseases like COVID, flu, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios the data was increased by 100x:\n",
    "    - For the case that need to be met in terms of periodic timing, 100x is a really big volume to meet target be on time. For this scenarios, we should make design a region distributed big data for collecting, processing, modeling. This distributed big data should be many cluster on cloud-based system. Data will be split to chunks, every chunks being processed by a batch-job.\n",
    "    - For the case no worry about time, we can process the data as a single batch job. We could use existing cloud-based bigdata processing solution as datalake, datawarehouse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - For specific time update dashboard visualization, we can apply pipeline automation. Airflow is a good candidate cause of capacities schedule, automation, processing batch-job.\n",
    "    - The configurations of cloud-based processing system is a key factor to speed up the analysis progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    - We could consider publishing the parquet files to read-ony HDFS. \n",
    "    - In scenarios need  to meet high frequency SQL queries we consider place a midleware (ex. redis, memcahe, memsql) layer in the front of database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
