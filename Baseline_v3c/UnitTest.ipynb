{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",AWS_SECRET_ACCESS_KEY)\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure read out validation pair values from SAS Labels Description\n",
    "def get_validation_code_from_SAS_labels(sas_input_label):\n",
    "    '''\n",
    "    This procedure read a input SAS Labels Description and then write out validation code datasets.\n",
    "    The SAS Labels Description included validation code datasets with labels: I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sas_input_label : string\n",
    "        The label name of validation code dataset. Its can be one of I94RES (same to I94CIT), I94PORT, I94ADDR, I94MODE, I94VISA.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    validation_code_list : validation_value_pairs(tuple(str_valid_code, str_valid_value))\n",
    "        The return output is a specific SAS label list of validation code value pairs.\n",
    "    '''\n",
    "\n",
    "    # Read input SAS Labels Descriptions\n",
    "    with open('I94_SAS_Labels_Descriptions.SAS') as sas_validation_code:\n",
    "            labels_from_sas = sas_validation_code.read()\n",
    "\n",
    "    # Parse labels from SAS Label Description input\n",
    "    sas_labels = labels_from_sas[labels_from_sas.index(sas_input_label):]\n",
    "    sas_labels = sas_labels[:sas_labels.index(';')]\n",
    "    \n",
    "    # Processing line by line, remove separate charaters and then append value pair\n",
    "    lines = sas_labels.splitlines()\n",
    "    validation_code_list = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            valid_code, valid_value = line.split('=')\n",
    "            valid_code = valid_code.strip().strip(\"'\").strip('\"')\n",
    "            valid_value = valid_value.strip().strip(\"'\").strip('\"').strip()\n",
    "            validation_code_list.append((valid_code, valid_value))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return validation_code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure extract parts from SAS Labels Description\n",
    "def extract_staging_sas_label(label):\n",
    "    '''\n",
    "    This procedure get a specific part of data dictionary from SAS_Labels_Descriptions.SAS to a schema.\n",
    "    The dictionary part include valid_code and valid_value\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    label: \n",
    "        a string input of specific label from \"SAS_Label_Descriptions.SAS\"\n",
    "        \n",
    "    Syntax note: \n",
    "        input value in string datatype, need inside a pair of single quotes. Ex: 'I94RES', 'I94PORTS'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dir of csv files with a specific part as input label.\n",
    "    '''\n",
    "    label_name = label\n",
    "    valid_code = label + \"_valid_code\"\n",
    "    valid_value = label + \"_valid_value\"\n",
    "    csv_output = label + \"_sas_label_validation\"\n",
    "    \n",
    "    # Create dir for output\n",
    "    parent_dir = \"./\"\n",
    "    path = os.path.join(parent_dir, csv_output)\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        print(\"Directory '%s' created successfully\" % csv_output)\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\" % csv_output)\n",
    "\n",
    "    # Define output dataframe structure\n",
    "    schema = R([\n",
    "        Fld(valid_code, Str()),\n",
    "        Fld(valid_value, Str())\n",
    "    ])\n",
    "\n",
    "    # Create dataframe from extracted label\n",
    "    df = spark.createDataFrame(\n",
    "        data=get_validation_code_from_SAS_labels(label_name),\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # df.write.mode('overwrite').csv(csv_output)\n",
    "    shutil.rmtree(csv_output, ignore_errors=False, onerror=None)\n",
    "    df.write.options(header='True', delimiter=',').csv(csv_output)\n",
    "\n",
    "    df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(csv_output)\n",
    "\n",
    "    print(\"Top 20 rows of {} \".format(csv_output))\n",
    "    df.show()\n",
    "\n",
    "    print(\"Count rows of {}: {} \".format(csv_output, df.count()))\n",
    "    \n",
    "    print(\"Check unique value of {}: {} \".format(csv_output, df.select(valid_code).distinct().count()))\n",
    "\n",
    "    print(\"Staging csv files in: {}\".format(csv_output))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure convert column name\n",
    "def convert_column_names(df):\n",
    "    '''\n",
    "    This procedure standardizing column names to snake case format. Format ex: customer_name, billing_address, total_price.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : string_of_dataframe\n",
    "        The input dataframe with column names might have elements of messy columns names, including accents, different delimiters, casing and multiple white spaces.\n",
    "        Snake case style replaces the white spaces and symbol delimiters with underscore and converts all characters to lower case\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with column names has been changed to snake_case format.\n",
    "    '''\n",
    "    cols = df.columns\n",
    "    column_name_changed = []\n",
    "\n",
    "    for col in cols:\n",
    "        new_column = col.lstrip().rstrip().lower().replace (\" \", \"_\").replace (\"-\", \"_\")\n",
    "        column_name_changed.append(new_column)\n",
    "\n",
    "    df.columns = column_name_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Procedure remove specific dir (if need)\n",
    "def rmdir(directory):\n",
    "    '''\n",
    "    This procedure perform pure recursive a directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string_of_path_to_dir\n",
    "        The input directory is a path to target dir. This dir and all its belong child objects wil be deleted.\n",
    "        Syntax note: rmdir(Path(\"target_path_to_dir\"))\n",
    "            with Path(\"target_path_to_dir\") returns path to dir format as 'directory' input\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    directory = Path(directory)\n",
    "    for item in directory.iterdir():\n",
    "        if item.is_dir():\n",
    "            rmdir(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "    directory.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read input dataset to Spark dataframe\n",
    "# i94immi_dataset = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# i94_immi_df = spark.read.format('com.github.saurfang.sas.spark').load(i94immi_dataset)\n",
    "i94immi_df = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data type\n",
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Attributes columns\n",
    "i94immi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94immi_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create tableview for data manipulation\n",
    "i94immi_df.createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop amount of un-makesance records cause by `DepartureDate >= ArrivalDate`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE arrdate <= depdate\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add column `arrival_date = timestone + arrdate_offset_day`, with:\n",
    "# - timestone = '1960-01-01' (***datetime*** datatype)\n",
    "# - arrdate_offset_day = 'arrdate' (***integer*** datatype)\n",
    "# - arrival_date (***datetime*** datatype)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date \n",
    "    FROM i94immi_table\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add column `departure_date = timestone + depdate_offset_day`, with:\n",
    "# - `timestone` = '1960-01-01' (***datetime*** datatype)\n",
    "# - `depdate_offset_day` = 'depdate' (***integer*** datatype)\n",
    "# - `departure_date` (***datetime*** datatype)\n",
    "spark.sql(\"\"\"SELECT *, CASE \n",
    "                        WHEN depdate >= arrdate THEN date_add(to_date('1960-01-01'), depdate)\n",
    "                        WHEN depdate IS NULL THEN NULL\n",
    "                        ELSE 'NaN' END AS departure_date \n",
    "                FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracted i94mode from `I94_SAS_Labels_Descriptions_SAS`\n",
    "# i94mode includes:\n",
    "# {'1': 'Air', '2': 'Sea', '3': 'Land', '9': 'Not reported'}\n",
    "# Keep air arrival only, mean keep `i94mode=1`\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94mode == 1.0\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Mapping `i94visa` numbers to `visatype` instead\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *, CASE \n",
    "                    WHEN i94visa = 1.0 THEN 'Business' \n",
    "                    WHEN i94visa = 2.0 THEN 'Pleasure'\n",
    "                    WHEN i94visa = 3.0 THEN 'Student'\n",
    "                    ELSE 'NaN' END AS visa_type\n",
    "        FROM i94immi_table\n",
    "    \"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep user records of `male = 'M'` and `female = 'F'` only\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM i94immi_table \n",
    "    WHERE gender IN ('F', 'M')\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop NULL value on arrival state\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "    WHERE i94addr IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView(\"i94immi_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keep necessary columns\n",
    "# Convert month and year timestamp\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cicid,\n",
    "            i94cit,\n",
    "            i94res,\n",
    "            i94port,\n",
    "            arrival_date,\n",
    "            YEAR(arrival_date) as i94yr,\n",
    "            MONTH(arrival_date) as i94mon,\n",
    "            i94mode,\n",
    "            i94addr,\n",
    "            departure_date,\n",
    "            i94bir,\n",
    "            i94visa,\n",
    "            count,\n",
    "            dtadfile,\n",
    "            biryear,\n",
    "            dtaddto,\n",
    "            gender,\n",
    "            insnum,\n",
    "            airline,\n",
    "            admnum,\n",
    "            fltno,\n",
    "            visatype,\n",
    "            visa_type\n",
    "        FROM i94immi_table\n",
    "            \"\"\").createOrReplaceTempView('i94immi_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify the cleaned\n",
    "i94immi_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM i94immi_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|arrival_date|i94yr|i94mon|i94mode|i94addr|departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     CA|    2016-05-08|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     NV|    2016-05-17|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|  2016-04-30| 2016|     4|    1.0|     WA|    2016-05-08|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+------------+-----+------+-------+-------+--------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94immi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|amount_i94immi_rows|\n",
      "+-------------------+\n",
      "|            2377896|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the cleaned\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as amount_i94immi_rows\n",
    "    FROM i94immi_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './i94immi_df_clean' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Staging to csv file --> write a function write spark sql dataframe to csv\n",
    "path = './i94immi_df_clean'\n",
    "try:\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    print(\"Directory '%s' created successfully\" % path)\n",
    "except OSError as error:\n",
    "    print(\"Directory '%s' can not be created\" % path)\n",
    "\n",
    "rmdir(Path(\"i94immi_df_clean\")) # use this line from 2nd running\n",
    "i94immi_df.write.options(header='True', delimiter=',').csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Read out from staging csv to test S3 storage parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify out from staging csv partitions\n",
    "i94immi_df = spark.read.options(inferSchema=\"true\", delimiter=\",\", header = \"true\").csv(\"i94immi_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|    cicid|i94cit|i94res|i94port|       arrival_date|i94yr|i94mon|i94mode|i94addr|     departure_date|i94bir|i94visa|count|dtadfile|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|visa_type|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "|5748517.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     CA|2016-05-08 00:00:00|  40.0|    1.0|  1.0|20160430| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1| Business|\n",
      "|5748518.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     NV|2016-05-17 00:00:00|  32.0|    1.0|  1.0|20160430| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1| Business|\n",
      "|5748519.0| 245.0| 438.0|    LOS|2016-04-30 00:00:00| 2016|     4|    1.0|     WA|2016-05-08 00:00:00|  29.0|    1.0|  1.0|20160430| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1| Business|\n",
      "+---------+------+------+-------+-------------------+-----+------+-------+-------+-------------------+------+-------+-----+--------+-------+--------+------+------+-------+--------------+-----+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare to save out s3 storage\n",
    "i94immi_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'double'),\n",
       " ('i94cit', 'double'),\n",
       " ('i94res', 'double'),\n",
       " ('i94port', 'string'),\n",
       " ('arrival_date', 'timestamp'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94mode', 'double'),\n",
       " ('i94addr', 'string'),\n",
       " ('departure_date', 'timestamp'),\n",
       " ('i94bir', 'double'),\n",
       " ('i94visa', 'double'),\n",
       " ('count', 'double'),\n",
       " ('dtadfile', 'int'),\n",
       " ('biryear', 'double'),\n",
       " ('dtaddto', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'double'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string'),\n",
       " ('visa_type', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94immi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change to Spark SQl\n",
    "i94immi_df.createOrReplaceTempView('fact_i94immi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact table\n",
    "fact_i94immi = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            cicid as travel_cicid,\n",
    "            i94cit as from_country_code,\n",
    "            i94res as immi_country_code,\n",
    "            i94port as arrival_port_code,\n",
    "            arrival_date as immi_arrival_date,\n",
    "            i94yr as arrival_year,\n",
    "            i94mon as arrival_month,\n",
    "            i94mode as airline_mode_code,\n",
    "            i94addr as immi_state_code,\n",
    "            departure_date,\n",
    "            i94bir as traveller_age,\n",
    "            i94visa as visatype_by_number,\n",
    "            biryear as traveller_birth_year,\n",
    "            gender as traveller_sex,\n",
    "            fltno as immi_flight_code,\n",
    "            visatype as visatype_by_code,\n",
    "            visa_type\n",
    "        FROM fact_i94immi\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://capstone-project-outputs/s3_outputs/unitTest_fact_i94immi.parquet'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 URI `s3://capstone-project-outputs/s3_outputs/`\n",
    "# Copy S3 URI = 's3a://dendcapstonetest/s3_outputs/'\n",
    "# update this to your bucket name\n",
    "s3_uri = 's3://capstone-project-outputs/s3_outputs/'\n",
    "parquet_outputs = s3_uri + 'unitTest_fact_i94immi.parquet'\n",
    "parquet_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o352.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ed6007b193fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfact_i94immi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o352.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "fact_i94immi.write.mode(\"overwrite\").parquet(parquet_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
