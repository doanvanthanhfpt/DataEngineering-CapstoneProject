{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,\\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int,\\\n",
    "    TimestampType as Timestamp, DateType as Date, LongType as Long\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",AWS_SECRET_ACCESS_KEY)\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectations with datasets from data modeling fact&dim tables:\n",
    "- Relationship between amount of travel immigration and weather duration by month of city.\n",
    "- Relationship between specific visa type used for a specific city immigration.\n",
    "- Airline statistic traffic to specific city.\n",
    "- Ranking US immigration volume from other countries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.).\n",
    "* Unit tests for the scripts to ensure they are doing the right thing.\n",
    "* Source/Count checks to ensure completeness.\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parquet files of fact & dim tables\n",
    "parquet_outputs = './ws_parquet_outputs'\n",
    "\n",
    "fact_i94immi_parquet_outputs = parquet_outputs + '/fact_i94immi.parquet'\n",
    "dim_visa_parquet_outputs = parquet_outputs + '/dim_visa.parquet'\n",
    "dim_immi_flight_parquet_outputs = parquet_outputs + '/dim_immi_flight.parquet'\n",
    "dim_immi_travaller_parquet_outputs = parquet_outputs + '/dim_immi_travaller.parquet'\n",
    "fact_worldtempe_parquet_outputs = parquet_outputs + '/fact_worldtempe.parquet'\n",
    "dim_i94port_parquet_outputs = parquet_outputs + '/dim_i94port.parquet'\n",
    "dim_i94addr_parquet_outputs = parquet_outputs + '/dim_i94addr.parquet'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View table structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_i94immi_parquet_outputs_df = spark.read.parquet(fact_i94immi_parquet_outputs)\n",
    "fact_i94immi_parquet_outputs_df.createOrReplaceTempView('fact_i94immi')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM fact_i94immi\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_visa_parquet_outputs_df = spark.read.parquet(dim_visa_parquet_outputs)\n",
    "dim_visa_parquet_outputs_df.createOrReplaceTempView('dim_visa')\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_visa\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immi_flight_parquet_outputs_df = spark.read.parquet(dim_immi_flight_parquet_outputs)\n",
    "dim_immi_flight_parquet_outputs_df.createOrReplaceTempView('dim_immi_flight')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_immi_flight\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_immi_travaller_parquet_outputs_df = spark.read.parquet(dim_immi_travaller_parquet_outputs)\n",
    "dim_immi_travaller_parquet_outputs_df.createOrReplaceTempView('dim_immi_travaller')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_immi_travaller\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_worldtempe_parquet_outputs_df = spark.read.parquet(fact_worldtempe_parquet_outputs)\n",
    "fact_worldtempe_parquet_outputs_df.createOrReplaceTempView('fact_worldtempe')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM fact_worldtempe\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_i94port_parquet_outputs_df = spark.read.parquet(dim_i94port_parquet_outputs)\n",
    "dim_i94port_parquet_outputs_df.createOrReplaceTempView('dim_i94port')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_i94port\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_i94addr_parquet_outputs_df = spark.read.parquet(dim_i94addr_parquet_outputs)\n",
    "dim_i94addr_parquet_outputs_df.createOrReplaceTempView('dim_i94addr')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM dim_i94addr\n",
    "    \"\"\").show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Primarykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_visa\n",
    "total_column = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM dim_visa\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim_visa verify duplicate rows by 'airline' and 'fltno'\n",
    "distinct_column = dim_visa_parquet_outputs_df.select(['airline', 'fltno']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Result\n",
    "if total_column == distinct_column:\n",
    "    print(\"Total rows ({}) = unique rows ({})\".format(total_column, distinct_column))\n",
    "    print(\"Dim table ok\")\n",
    "else:\n",
    "    print(\"Dim table not consistence!!! Please check again.!!!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
