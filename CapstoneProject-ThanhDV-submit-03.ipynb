{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "A core responsibility of The National Travel and Tourism Office (NTTO) is to collect, analyze, and disseminate international travel and tourism statistics. \n",
    "\n",
    "NTTO's Board of Managers are charged with managing, improving, and expanding the system to fully account and report the impact of travel and tourism in the United States. The analysis results help to forcecast and operation, support make decision creates a positive climate for growth in travel and tourism by reducing institutional barriers to tourism, administers joint marketing efforts, provides official travel and tourism statistics, and coordinates efforts across federal agencies.\n",
    "\n",
    "##### Project Description\n",
    "\n",
    "In this project, some source datas will be use to do data modeling:\n",
    "* **I94 Immigration**: The source data for I94 immigration data is available in local disk in the format of sas7bdat. This data comes from US National Tourism and Trade Office. The data dictionary is also included in this project for reference. The actual source of the data is from https://travel.trade.gov/research/reports/i94/historical/2016.html. This data is already uploaded to the workspace.\n",
    "\n",
    "* **World Temperature Data**: This dataset came from Kaggle. This data is already uploaded to the workspace.\n",
    "\n",
    "* **Airport Code**: This is a simple table with airport codes. The source of this data is from https://datahub.io/core/airport-codes#data. It is highly recommended to use it for educational purpose only but not for commercial or any other purpose. This data is already uploaded to the workspace.\n",
    "\n",
    "* **U.S. City Demographic Data**: This data comes from OpenSoft link https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "* Other validation text files such as *I94Addr.txt*, *I94CIT_I94RES.txt*, *I94Mode.txt*, *I94Port.txt** and **I94Visa.txt* extracted from the *I94_SAS_Labels_Descriptions.SAS*.\n",
    "\n",
    "##### The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make decision of project scope and the technical step solution we do data assessment on datasets:\n",
    "* I94 Immigration\n",
    "* World Temperature Data\n",
    "* U.S. City Demographic Data\n",
    "* Airport Code\n",
    "* Other reference *I94_SAS_Labels_Descriptions.SAS*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project purpose are analysis outputs of:\n",
    "- The relation between amount of travel immigration with weather duration of US regions.\n",
    "- The relation between amount of travel immigration with US airports.\n",
    "- The relation between amount of travel immigration with city demographics.\n",
    "- About immigration type statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools will be used and import:\n",
    "- Spark\n",
    "- Python and its modules\n",
    "- Pandas\n",
    "- AWS Redshift cluster will be considered as an optional to run one or more ETL steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here - Done\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import re\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create config file *etl.cfg* for path parameters:\n",
    "~~~\n",
    "[DIR]\n",
    "INPUT_DIR = .\n",
    "OUTPUT_DIR = ./storage\n",
    "\n",
    "[DATA]\n",
    "I94_IMMI = ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
    "WORLD_TEMPE = ../../data2/GlobalLandTemperaturesByCity.csv\n",
    "CITY_DEMOGRAPHIC = ./us-cities-demographics.csv\n",
    "AIR_PORT = ./airport-codes_csv.csv\n",
    "\n",
    "[SPLIT]\n",
    "I94_IMMI_SPLITED_DIR = ./storage/.sas7bdat\n",
    "WORLD_TEMPE_SPLITED_DIR = ./storage/.csv\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse config file for path configurations - Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('etl.cfg')\n",
    "\n",
    "input_data_source = config.get('DIR','INPUT_DIR')\n",
    "output_processed_data = config.get('DIR','OUTPUT_DIR')\n",
    "\n",
    "i94immi_dataset = config.get('DATA','I94_IMMI')\n",
    "worldtempe_dataset = config.get('DATA','WORLD_TEMPE')\n",
    "citydemo_dataset = config.get('DATA','CITY_DEMOGRAPHIC')\n",
    "airport_dataset = config.get('DATA','AIR_PORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on production only\n",
    "spark = SparkSession.builder\\\n",
    "            .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "            .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run on production only\n",
    "local_spark = SparkSession.builder.appName(\"localSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe and Gather Data\n",
    "\n",
    "Take a look on datasets description includes schema, sample records, number of rows, attributes, number of data file (if need).  And then choose datasets will be using for data modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset assessment by counting number of records and data size. Take a look on schema, data column structure, attributes and some of sample records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Airport Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract dictionary informations from *I94_SAS_Labels_Descriptions.SAS*.\n",
    "    - I94CIT & I94RES --> i94cntyl.txt\n",
    "    - I94PORT --> i94prtl.txt\n",
    "    - I94MODE --> i94model.txt\n",
    "    - I94ADDR --> i94addrl\n",
    "    - I94VISA --> i94visa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mapper(file, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94_cit_and_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94_port = code_mapper(f_content, \"i94prtl\")\n",
    "i94_mode = code_mapper(f_content, \"i94model\")\n",
    "i94_addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94_visa = {'1':'Business',\n",
    "            '2': 'Pleasure',\n",
    "            '3' : 'Student'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_city_to_i94port(city):\n",
    "    results = [v for k, v in i94_port.items() if re.match(city, k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "convert_city_to_i94portUDF = udf(lambda z:convert_city_to_i94port(z))\n",
    "\n",
    "temp_df_final = temp_df.withColumn('i94_port', convert_city_to_i94portUDF(col(\"city\")))\n",
    "temp_df_final.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of analysis step targets:\n",
    "- Staging datasets *Immigration Data*, *Temperature Data*, *U.S. City Demographic Data* and *Airport Code Table*.\n",
    "- Create data modeling diagram as a star schema.\n",
    "- Create and load data to fact table and dimension tables.\n",
    "- Check data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset quality and validation issues recognition from gathering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I94 Immigration\n",
    "    - Re-structure columns, convert column name.\n",
    "    - Check and drop dupllicate for index column and importance columns.\n",
    "    - Clear emply cell\n",
    "    - Clear data in wrong format\n",
    "    - Clear wrong data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- World Temperature Data\n",
    "    - Convert data type of column 'dt' to datetime\n",
    "    - Filter to keep US in column \"Country\" only\n",
    "    - Re-structure columns, convert column name.\n",
    "    - Check and drop dupllicate for index column and importance columns: dt\n",
    "    - Clear emply cell\n",
    "    - Clear data in wrong format\n",
    "    - Clear wrong data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U.S. City Demographic Data\n",
    "    - Re-structure columns, convert column name.\n",
    "    - Change population values percentage values.\n",
    "    - Check unique values of \"City\" and \"State\" and drop duplicate.\n",
    "    - Clear emply cell\n",
    "    - Drop duplicated records\n",
    "    - Clear data in wrong format\n",
    "    - Clear wrong data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Airport Code\n",
    "    - Drop records with non-US values on column \"Country\".\n",
    "    - Re-structure columns, convert column name.\n",
    "    - Clear emply cell\n",
    "    - Drop duplicated records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other reference *I94_SAS_Labels_Descriptions.SAS*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parse *I94_SAS_Labels_Descriptions.SAS*. for validations on staging steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging I94 Immigration include columns:\n",
    "    - i94cit = code for visitor origin country\n",
    "    - i94port = code for destination USA city\n",
    "    - arrdate = arrival date in the USA\n",
    "    - i94mode = code for transportation mode\n",
    "    - depdate = departure date from the USA\n",
    "    - i94visa = code for visa type (reason for visiting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging World Temperature Data include columns:\n",
    "    - Date\n",
    "    - AverageTemperature\n",
    "    - City\n",
    "    - State\n",
    "    - Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging U.S. City Demographic Data include columns:\n",
    "    - City\n",
    "    - State\n",
    "    - Median Age\n",
    "    - Male Population\n",
    "    - Female Population\n",
    "    - Total Population\n",
    "    - Number of Veterans\n",
    "    - Foreign-born\n",
    "    - Average Household Size\n",
    "    - Race\n",
    "    - Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning and staging  Airport Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Conceptual Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start schema diagram transformed\n",
    "- Start_schema_diagram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table:\n",
    "- Fact table fact_i94visits will contain informations from the i94 immigration data joined with daily average temperature on the port city and arrival date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension table:\n",
    "- dim_us_ports contains informations like US port of entry code, city, state code and state name.\n",
    "- dim_visa maps visa type which gives information like reason for visiting.\n",
    "- dim_countries tells which country does visitor come from.\n",
    "- dim_travelmode gives mode of transportation: air, land or sea.\n",
    "- dim_demographicscontains median age and population informations about US port city.\n",
    "- dim_date contains date information like year, month, day, week of year and weekday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Mapping Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline steps are described below:\n",
    "- Load raw dataset from source into Spark dataframe: df_spark_i94, df_spark_dem and df_spark_temp for one month.\n",
    "- Clean each Spark dataframe as decscibed in *Step 2 Cleaning steps* and write each cleaned dataframe into parquet as staging table: stage_i94_immigration, stage_cities_demographics and stage_uscities_temperatures.\n",
    "- Create and load dimension tables: dim_us_ports, dim_visa, dim_countries, dim_travelmode and dim_demographics.\n",
    "- Create and load fact table fact_i94_visits joining stage_i94_immigration and stage_uscities_temperatures.\n",
    "- Create and load dimension tables and dim_date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Run Pipelines to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "* Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "* Unit tests for the scripts to ensure they are doing the right thing\n",
    "* Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "* i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration\n",
    "* AverageTemperature = average temperature of destination city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * Spark was chosen since it can easily handle multiple file formats (including SAS) containing large amounts of data. \n",
    "    * Spark SQL was chosen to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Propose how often the data should be updated and why.\n",
    "    * The data should be updated monthly in conjunction with the current raw file format.\n",
    "    * Case 2\n",
    "    * Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    * The data was increased by 100x.\n",
    "    * If the data was increased by 100x, we would no longer process the data as a single batch job. We could perhaps do incremental updates using a tool such as Uber's Hudi. We could also consider moving Spark to cluster mode using a cluster manager such as Yarn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * If the data needs to populate a dashboard daily to meet an SLA then we could use a scheduling tool such as Airflow to run the ETL pipeline overnight.\n",
    "    * Others solution ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The database needed to be accessed by 100+ people.\n",
    "    * If the database needed to be accessed by 100+ people, we could consider publishing the parquet files to HDFS and giving read access to users that need it. \n",
    "    * If the users want to run SQL queries on the raw data, we could consider publishing to HDFS using a tool such as Impala."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.test_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "af440d615888d659e90fddc0bb1ee33d8fb1bc777d369b970824ddfe4cd12e65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
